// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See License.txt in the project root for license information.
// Code generated by Microsoft (R) AutoRest Code Generator. DO NOT EDIT.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.

package azsearch

import "time"

// ASCIIFoldingTokenFilter - Converts alphabetic, numeric, and symbolic Unicode characters which are not in the first 127
// ASCII characters (the "Basic Latin" Unicode block) into their ASCII equivalents, if such equivalents exist.
// This token filter is implemented using Apache Lucene.
type ASCIIFoldingTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// A value indicating whether the original token will be kept. Default is false.
	PreserveOriginal *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type ASCIIFoldingTokenFilter.
func (a *ASCIIFoldingTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: a.Name,
		ODataType: a.ODataType,
	}
}

// AnalyzeRequest - Specifies some text and analysis components used to break that text into tokens.
type AnalyzeRequest struct {
	// REQUIRED; The text to break into tokens.
	Text *string

	// The name of the analyzer to use to break the given text. If this parameter is not specified, you must specify a tokenizer
// instead. The tokenizer and analyzer parameters are mutually exclusive.
	Analyzer *LexicalAnalyzerName

	// An optional list of character filters to use when breaking the given text. This parameter can only be set when using the
// tokenizer parameter.
	CharFilters []*CharFilterName

	// An optional list of token filters to use when breaking the given text. This parameter can only be set when using the tokenizer
// parameter.
	TokenFilters []*TokenFilterName

	// The name of the tokenizer to use to break the given text. If this parameter is not specified, you must specify an analyzer
// instead. The tokenizer and analyzer parameters are mutually exclusive.
	Tokenizer *LexicalTokenizerName
}

// AnalyzeResult - The result of testing an analyzer on text.
type AnalyzeResult struct {
	// REQUIRED; The list of tokens returned by the analyzer specified in the request.
	Tokens []*AnalyzedTokenInfo
}

// AnalyzedTokenInfo - Information about a token returned by an analyzer.
type AnalyzedTokenInfo struct {
	// READ-ONLY; The index of the last character of the token in the input text.
	EndOffset *int32

	// READ-ONLY; The position of the token in the input text relative to other tokens. The first token in the input text has
// position 0, the next has position 1, and so on. Depending on the analyzer used, some tokens
// might have the same position, for example if they are synonyms of each other.
	Position *int32

	// READ-ONLY; The index of the first character of the token in the input text.
	StartOffset *int32

	// READ-ONLY; The token returned by the analyzer.
	Token *string
}

// AzureActiveDirectoryApplicationCredentials - Credentials of a registered application created for your search service, used
// for authenticated access to the encryption keys stored in Azure Key Vault.
type AzureActiveDirectoryApplicationCredentials struct {
	// REQUIRED; An AAD Application ID that was granted the required access permissions to the Azure Key Vault that is to be used
// when encrypting your data at rest. The Application ID should not be confused with the
// Object ID for your AAD Application.
	ApplicationID *string

	// The authentication key of the specified AAD application.
	ApplicationSecret *string
}

// AzureOpenAIEmbeddingSkill - Allows you to generate a vector embedding for a given text input using the Azure OpenAI resource.
type AzureOpenAIEmbeddingSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// API key of the designated Azure OpenAI resource.
	APIKey *string

	// The user-assigned managed identity used for outbound connections.
	AuthIdentity IndexerDataIdentityClassification

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// ID of the Azure OpenAI model deployment on the designated resource.
	DeploymentName *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models.
	Dimensions *int32

	// The name of the embedding model that is deployed at the provided deploymentId path.
	ModelName *AzureOpenAIModelName

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string

	// The resource URI of the Azure OpenAI resource.
	ResourceURI *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type AzureOpenAIEmbeddingSkill.
func (a *AzureOpenAIEmbeddingSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: a.Context,
		Description: a.Description,
		Inputs: a.Inputs,
		Name: a.Name,
		ODataType: a.ODataType,
		Outputs: a.Outputs,
	}
}

// AzureOpenAIParameters - Specifies the parameters for connecting to the Azure OpenAI resource.
type AzureOpenAIParameters struct {
	// API key of the designated Azure OpenAI resource.
	APIKey *string

	// The user-assigned managed identity used for outbound connections.
	AuthIdentity IndexerDataIdentityClassification

	// ID of the Azure OpenAI model deployment on the designated resource.
	DeploymentName *string

	// The name of the embedding model that is deployed at the provided deploymentId path.
	ModelName *AzureOpenAIModelName

	// The resource URI of the Azure OpenAI resource.
	ResourceURI *string
}

// AzureOpenAIVectorizer - Specifies the Azure OpenAI resource used to vectorize a query string.
type AzureOpenAIVectorizer struct {
	// REQUIRED; The name of the kind of vectorization method being configured for use with vector search.
	Kind *VectorSearchVectorizerKind

	// REQUIRED; The name to associate with this particular vectorization method.
	Name *string

	// Contains the parameters specific to Azure OpenAI embedding vectorization.
	AzureOpenAIParameters *AzureOpenAIParameters
}

// GetVectorSearchVectorizer implements the VectorSearchVectorizerClassification interface for type AzureOpenAIVectorizer.
func (a *AzureOpenAIVectorizer) GetVectorSearchVectorizer() *VectorSearchVectorizer {
	return &VectorSearchVectorizer{
		Kind: a.Kind,
		Name: a.Name,
	}
}

// BM25Similarity - Ranking function based on the Okapi BM25 similarity algorithm. BM25 is a TF-IDF-like algorithm that includes
// length normalization (controlled by the 'b' parameter) as well as term frequency saturation
// (controlled by the 'k1' parameter).
type BM25Similarity struct {
	// REQUIRED
	ODataType *string

	// This property controls how the length of a document affects the relevance score. By default, a value of 0.75 is used. A
// value of 0.0 means no length normalization is applied, while a value of 1.0
// means the score is fully normalized by the length of the document.
	B *float64

	// This property controls the scaling function between the term frequency of each matching terms and the final relevance score
// of a document-query pair. By default, a value of 1.2 is used. A value of 0.0
// means the score does not scale with an increase in term frequency.
	K1 *float64
}

// GetSimilarity implements the SimilarityClassification interface for type BM25Similarity.
func (b *BM25Similarity) GetSimilarity() *Similarity {
	return &Similarity{
		ODataType: b.ODataType,
	}
}

// BinaryQuantizationCompressionConfiguration - Contains configuration options specific to the binary quantization compression
// method used during indexing and querying.
type BinaryQuantizationCompressionConfiguration struct {
	// REQUIRED; The name of the kind of compression method being configured for use with vector search.
	Kind *VectorSearchCompressionKind

	// REQUIRED; The name to associate with this particular configuration.
	Name *string

	// Default oversampling factor. Oversampling will internally request more documents (specified by this multiplier) in the
// initial search. This increases the set of results that will be reranked using
// recomputed similarity scores from full-precision vectors. Minimum value is 1, meaning no oversampling (1x). This parameter
// can only be set when rerankWithOriginalVectors is true. Higher values improve
// recall at the expense of latency.
	DefaultOversampling *float64

	// If set to true, once the ordered set of results calculated using compressed vectors are obtained, they will be reranked
// again by recalculating the full-precision similarity scores. This will improve
// recall at the expense of latency.
	RerankWithOriginalVectors *bool
}

// GetVectorSearchCompressionConfiguration implements the VectorSearchCompressionConfigurationClassification interface for
// type BinaryQuantizationCompressionConfiguration.
func (b *BinaryQuantizationCompressionConfiguration) GetVectorSearchCompressionConfiguration() *VectorSearchCompressionConfiguration {
	return &VectorSearchCompressionConfiguration{
		DefaultOversampling: b.DefaultOversampling,
		Kind: b.Kind,
		Name: b.Name,
		RerankWithOriginalVectors: b.RerankWithOriginalVectors,
	}
}

// CharFilter - Base type for character filters.
type CharFilter struct {
	// REQUIRED; The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of char filter.
	ODataType *string
}

// GetCharFilter implements the CharFilterClassification interface for type CharFilter.
func (c *CharFilter) GetCharFilter() *CharFilter { return c }

// CjkBigramTokenFilter - Forms bigrams of CJK terms that are generated from the standard tokenizer. This token filter is
// implemented using Apache Lucene.
type CjkBigramTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The scripts to ignore.
	IgnoreScripts []*CjkBigramTokenFilterScripts

	// A value indicating whether to output both unigrams and bigrams (if true), or just bigrams (if false). Default is false.
	OutputUnigrams *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type CjkBigramTokenFilter.
func (c *CjkBigramTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: c.Name,
		ODataType: c.ODataType,
	}
}

// ClassicSimilarity - Legacy similarity algorithm which uses the Lucene TFIDFSimilarity implementation of TF-IDF. This variation
// of TF-IDF introduces static document length normalization as well as coordinating factors
// that penalize documents that only partially match the searched queries.
type ClassicSimilarity struct {
	// REQUIRED
	ODataType *string
}

// GetSimilarity implements the SimilarityClassification interface for type ClassicSimilarity.
func (c *ClassicSimilarity) GetSimilarity() *Similarity {
	return &Similarity{
		ODataType: c.ODataType,
	}
}

// ClassicTokenizer - Grammar-based tokenizer that is suitable for processing most European-language documents. This tokenizer
// is implemented using Apache Lucene.
type ClassicTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that
// can be used is 300 characters.
	MaxTokenLength *int32
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type ClassicTokenizer.
func (c *ClassicTokenizer) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: c.Name,
		ODataType: c.ODataType,
	}
}

// CognitiveServicesAccount - Base type for describing any Azure AI service resource attached to a skillset.
type CognitiveServicesAccount struct {
	// REQUIRED; A URI fragment specifying the type of Azure AI service resource attached to a skillset.
	ODataType *string

	// Description of the Azure AI service resource attached to a skillset.
	Description *string
}

// GetCognitiveServicesAccount implements the CognitiveServicesAccountClassification interface for type CognitiveServicesAccount.
func (c *CognitiveServicesAccount) GetCognitiveServicesAccount() *CognitiveServicesAccount { return c }

// CognitiveServicesAccountKey - The multi-region account key of an Azure AI service resource that's attached to a skillset.
type CognitiveServicesAccountKey struct {
	// REQUIRED; The key used to provision the Azure AI service resource attached to a skillset.
	Key *string

	// REQUIRED; A URI fragment specifying the type of Azure AI service resource attached to a skillset.
	ODataType *string

	// Description of the Azure AI service resource attached to a skillset.
	Description *string
}

// GetCognitiveServicesAccount implements the CognitiveServicesAccountClassification interface for type CognitiveServicesAccountKey.
func (c *CognitiveServicesAccountKey) GetCognitiveServicesAccount() *CognitiveServicesAccount {
	return &CognitiveServicesAccount{
		Description: c.Description,
		ODataType: c.ODataType,
	}
}

// CommonGramTokenFilter - Construct bigrams for frequently occurring terms while indexing. Single terms are still indexed
// too, with bigrams overlaid. This token filter is implemented using Apache Lucene.
type CommonGramTokenFilter struct {
	// REQUIRED; The set of common words.
	CommonWords []*string

	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// A value indicating whether common words matching will be case insensitive. Default is false.
	IgnoreCase *bool

	// A value that indicates whether the token filter is in query mode. When in query mode, the token filter generates bigrams
// and then removes common words and single terms followed by a common word.
// Default is false.
	UseQueryMode *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type CommonGramTokenFilter.
func (c *CommonGramTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: c.Name,
		ODataType: c.ODataType,
	}
}

// ConditionalSkill - A skill that enables scenarios that require a Boolean operation to determine the data to assign to an
// output.
type ConditionalSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type ConditionalSkill.
func (c *ConditionalSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: c.Context,
		Description: c.Description,
		Inputs: c.Inputs,
		Name: c.Name,
		ODataType: c.ODataType,
		Outputs: c.Outputs,
	}
}

// CorsOptions - Defines options to control Cross-Origin Resource Sharing (CORS) for an index.
type CorsOptions struct {
	// REQUIRED; The list of origins from which JavaScript code will be granted access to your index. Can contain a list of hosts
// of the form {protocol}://{fully-qualified-domain-name}[:{port#}], or a single '*' to
// allow all origins (not recommended).
	AllowedOrigins []*string

	// The duration for which browsers should cache CORS preflight responses. Defaults to 5 minutes.
	MaxAgeInSeconds *int64
}

// CustomAnalyzer - Allows you to take control over the process of converting text into indexable/searchable tokens. It's
// a user-defined configuration consisting of a single predefined tokenizer and one or more filters.
// The tokenizer is responsible for breaking text into tokens, and the filters for modifying tokens emitted by the tokenizer.
type CustomAnalyzer struct {
	// REQUIRED; The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of analyzer.
	ODataType *string

	// REQUIRED; The name of the tokenizer to use to divide continuous text into a sequence of tokens, such as breaking a sentence
// into words.
	Tokenizer *LexicalTokenizerName

	// A list of character filters used to prepare input text before it is processed by the tokenizer. For instance, they can
// replace certain characters or symbols. The filters are run in the order in which
// they are listed.
	CharFilters []*CharFilterName

	// A list of token filters used to filter out or modify the tokens generated by a tokenizer. For example, you can specify
// a lowercase filter that converts all characters to lowercase. The filters are run
// in the order in which they are listed.
	TokenFilters []*TokenFilterName
}

// GetLexicalAnalyzer implements the LexicalAnalyzerClassification interface for type CustomAnalyzer.
func (c *CustomAnalyzer) GetLexicalAnalyzer() *LexicalAnalyzer {
	return &LexicalAnalyzer{
		Name: c.Name,
		ODataType: c.ODataType,
	}
}

// CustomEntity - An object that contains information about the matches that were found, and related metadata.
type CustomEntity struct {
	// REQUIRED; The top-level entity descriptor. Matches in the skill output will be grouped by this name, and it should represent
// the "normalized" form of the text being found.
	Name *string

	// Defaults to false. Boolean value denoting whether comparisons with the entity name should be sensitive to accent.
	AccentSensitive *bool

	// An array of complex objects that can be used to specify alternative spellings or synonyms to the root entity name.
	Aliases []*CustomEntityAlias

	// Defaults to false. Boolean value denoting whether comparisons with the entity name should be sensitive to character casing.
// Sample case insensitive matches of "Microsoft" could be: microsoft,
// microSoft, MICROSOFT.
	CaseSensitive *bool

	// Changes the default accent sensitivity value for this entity. It be used to change the default value of all aliases accentSensitive
// values.
	DefaultAccentSensitive *bool

	// Changes the default case sensitivity value for this entity. It be used to change the default value of all aliases caseSensitive
// values.
	DefaultCaseSensitive *bool

	// Changes the default fuzzy edit distance value for this entity. It can be used to change the default value of all aliases
// fuzzyEditDistance values.
	DefaultFuzzyEditDistance *int32

	// This field can be used as a passthrough for custom metadata about the matched text(s). The value of this field will appear
// with every match of its entity in the skill output.
	Description *string

	// Defaults to 0. Maximum value of 5. Denotes the acceptable number of divergent characters that would still constitute a
// match with the entity name. The smallest possible fuzziness for any given match
// is returned. For instance, if the edit distance is set to 3, "Windows10" would still match "Windows", "Windows10" and "Windows
// 7". When case sensitivity is set to false, case differences do NOT count
// towards fuzziness tolerance, but otherwise do.
	FuzzyEditDistance *int32

	// This field can be used as a passthrough for custom metadata about the matched text(s). The value of this field will appear
// with every match of its entity in the skill output.
	ID *string

	// This field can be used as a passthrough for custom metadata about the matched text(s). The value of this field will appear
// with every match of its entity in the skill output.
	Subtype *string

	// This field can be used as a passthrough for custom metadata about the matched text(s). The value of this field will appear
// with every match of its entity in the skill output.
	Type *string
}

// CustomEntityAlias - A complex object that can be used to specify alternative spellings or synonyms to the root entity name.
type CustomEntityAlias struct {
	// REQUIRED; The text of the alias.
	Text *string

	// Determine if the alias is accent sensitive.
	AccentSensitive *bool

	// Determine if the alias is case sensitive.
	CaseSensitive *bool

	// Determine the fuzzy edit distance of the alias.
	FuzzyEditDistance *int32
}

// CustomEntityLookupSkill - A skill looks for text from a custom, user-defined list of words and phrases.
type CustomEntityLookupSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *CustomEntityLookupSkillLanguage

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// Path to a JSON or CSV file containing all the target text to match against. This entity definition is read at the beginning
// of an indexer run. Any updates to this file during an indexer run will not
// take effect until subsequent runs. This config must be accessible over HTTPS.
	EntitiesDefinitionURI *string

	// A global flag for AccentSensitive. If AccentSensitive is not set in CustomEntity, this value will be the default value.
	GlobalDefaultAccentSensitive *bool

	// A global flag for CaseSensitive. If CaseSensitive is not set in CustomEntity, this value will be the default value.
	GlobalDefaultCaseSensitive *bool

	// A global flag for FuzzyEditDistance. If FuzzyEditDistance is not set in CustomEntity, this value will be the default value.
	GlobalDefaultFuzzyEditDistance *int32

	// The inline CustomEntity definition.
	InlineEntitiesDefinition []*CustomEntity

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type CustomEntityLookupSkill.
func (c *CustomEntityLookupSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: c.Context,
		Description: c.Description,
		Inputs: c.Inputs,
		Name: c.Name,
		ODataType: c.ODataType,
		Outputs: c.Outputs,
	}
}

// DataChangeDetectionPolicy - Base type for data change detection policies.
type DataChangeDetectionPolicy struct {
	// REQUIRED; A URI fragment specifying the type of data change detection policy.
	ODataType *string
}

// GetDataChangeDetectionPolicy implements the DataChangeDetectionPolicyClassification interface for type DataChangeDetectionPolicy.
func (d *DataChangeDetectionPolicy) GetDataChangeDetectionPolicy() *DataChangeDetectionPolicy { return d }

// DataDeletionDetectionPolicy - Base type for data deletion detection policies.
type DataDeletionDetectionPolicy struct {
	// REQUIRED; A URI fragment specifying the type of data deletion detection policy.
	ODataType *string
}

// GetDataDeletionDetectionPolicy implements the DataDeletionDetectionPolicyClassification interface for type DataDeletionDetectionPolicy.
func (d *DataDeletionDetectionPolicy) GetDataDeletionDetectionPolicy() *DataDeletionDetectionPolicy { return d }

// DataSourceCredentials - Represents credentials that can be used to connect to a datasource.
type DataSourceCredentials struct {
	// The connection string for the datasource. Set to <unchanged> (with brackets) if you don't want the connection string updated.
// Set to <redacted> if you want to remove the connection string value from
// the datasource.
	ConnectionString *string
}

// DefaultCognitiveServicesAccount - An empty object that represents the default Azure AI service resource for a skillset.
type DefaultCognitiveServicesAccount struct {
	// REQUIRED; A URI fragment specifying the type of Azure AI service resource attached to a skillset.
	ODataType *string

	// Description of the Azure AI service resource attached to a skillset.
	Description *string
}

// GetCognitiveServicesAccount implements the CognitiveServicesAccountClassification interface for type DefaultCognitiveServicesAccount.
func (d *DefaultCognitiveServicesAccount) GetCognitiveServicesAccount() *CognitiveServicesAccount {
	return &CognitiveServicesAccount{
		Description: d.Description,
		ODataType: d.ODataType,
	}
}

// DictionaryDecompounderTokenFilter - Decomposes compound words found in many Germanic languages. This token filter is implemented
// using Apache Lucene.
type DictionaryDecompounderTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// REQUIRED; The list of words to match against.
	WordList []*string

	// The maximum subword size. Only subwords shorter than this are outputted. Default is 15. Maximum is 300.
	MaxSubwordSize *int32

	// The minimum subword size. Only subwords longer than this are outputted. Default is 2. Maximum is 300.
	MinSubwordSize *int32

	// The minimum word size. Only words longer than this get processed. Default is 5. Maximum is 300.
	MinWordSize *int32

	// A value indicating whether to add only the longest matching subword to the output. Default is false.
	OnlyLongestMatch *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type DictionaryDecompounderTokenFilter.
func (d *DictionaryDecompounderTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: d.Name,
		ODataType: d.ODataType,
	}
}

// DistanceScoringFunction - Defines a function that boosts scores based on distance from a geographic location.
type DistanceScoringFunction struct {
	// REQUIRED; A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64

	// REQUIRED; The name of the field used as input to the scoring function.
	FieldName *string

	// REQUIRED; Parameter values for the distance scoring function.
	Parameters *DistanceScoringParameters

	// REQUIRED; Indicates the type of function to use. Valid values include magnitude, freshness, distance, and tag. The function
// type must be lower case.
	Type *string

	// A value indicating how boosting will be interpolated across document scores; defaults to "Linear".
	Interpolation *ScoringFunctionInterpolation
}

// GetScoringFunction implements the ScoringFunctionClassification interface for type DistanceScoringFunction.
func (d *DistanceScoringFunction) GetScoringFunction() *ScoringFunction {
	return &ScoringFunction{
		Boost: d.Boost,
		FieldName: d.FieldName,
		Interpolation: d.Interpolation,
		Type: d.Type,
	}
}

// DistanceScoringParameters - Provides parameter values to a distance scoring function.
type DistanceScoringParameters struct {
	// REQUIRED; The distance in kilometers from the reference location where the boosting range ends.
	BoostingDistance *float64

	// REQUIRED; The name of the parameter passed in search queries to specify the reference location.
	ReferencePointParameter *string
}

// DocumentExtractionSkill - A skill that extracts content from a file within the enrichment pipeline.
type DocumentExtractionSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// A dictionary of configurations for the skill.
	Configuration map[string]any

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// The type of data to be extracted for the skill. Will be set to 'contentAndMetadata' if not defined.
	DataToExtract *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string

	// The parsingMode for the skill. Will be set to 'default' if not defined.
	ParsingMode *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type DocumentExtractionSkill.
func (d *DocumentExtractionSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: d.Context,
		Description: d.Description,
		Inputs: d.Inputs,
		Name: d.Name,
		ODataType: d.ODataType,
		Outputs: d.Outputs,
	}
}

// EdgeNGramTokenFilter - Generates n-grams of the given size(s) starting from the front or the back of an input token. This
// token filter is implemented using Apache Lucene.
type EdgeNGramTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The maximum n-gram length. Default is 2.
	MaxGram *int32

	// The minimum n-gram length. Default is 1. Must be less than the value of maxGram.
	MinGram *int32

	// Specifies which side of the input the n-gram should be generated from. Default is "front".
	Side *EdgeNGramTokenFilterSide
}

// GetTokenFilter implements the TokenFilterClassification interface for type EdgeNGramTokenFilter.
func (e *EdgeNGramTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: e.Name,
		ODataType: e.ODataType,
	}
}

// EdgeNGramTokenFilterV2 - Generates n-grams of the given size(s) starting from the front or the back of an input token.
// This token filter is implemented using Apache Lucene.
type EdgeNGramTokenFilterV2 struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32

	// The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32

	// Specifies which side of the input the n-gram should be generated from. Default is "front".
	Side *EdgeNGramTokenFilterSide
}

// GetTokenFilter implements the TokenFilterClassification interface for type EdgeNGramTokenFilterV2.
func (e *EdgeNGramTokenFilterV2) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: e.Name,
		ODataType: e.ODataType,
	}
}

// EdgeNGramTokenizer - Tokenizes the input from an edge into n-grams of the given size(s). This tokenizer is implemented
// using Apache Lucene.
type EdgeNGramTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32

	// The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32

	// Character classes to keep in the tokens.
	TokenChars []*TokenCharacterKind
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type EdgeNGramTokenizer.
func (e *EdgeNGramTokenizer) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: e.Name,
		ODataType: e.ODataType,
	}
}

// ElisionTokenFilter - Removes elisions. For example, "l'avion" (the plane) will be converted to "avion" (plane). This token
// filter is implemented using Apache Lucene.
type ElisionTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The set of articles to remove.
	Articles []*string
}

// GetTokenFilter implements the TokenFilterClassification interface for type ElisionTokenFilter.
func (e *ElisionTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: e.Name,
		ODataType: e.ODataType,
	}
}

// EntityLinkingSkill - Using the Text Analytics API, extracts linked entities from text.
type EntityLinkingSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified.
// If not set (default), or if explicitly set to null, all entities will be
// included.
	MinimumPrecision *float64

	// The version of the model to use when calling the Text Analytics service. It will default to the latest available when not
// specified. We recommend you do not specify this value unless absolutely
// necessary.
	ModelVersion *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type EntityLinkingSkill.
func (e *EntityLinkingSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: e.Context,
		Description: e.Description,
		Inputs: e.Inputs,
		Name: e.Name,
		ODataType: e.ODataType,
		Outputs: e.Outputs,
	}
}

// EntityRecognitionSkill - This skill is deprecated. Use the V3.EntityRecognitionSkill instead.
type EntityRecognitionSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// A list of entity categories that should be extracted.
	Categories []*EntityCategory

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *EntityRecognitionSkillLanguage

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// Determines whether or not to include entities which are well known but don't conform to a pre-defined type. If this configuration
// is not set (default), set to null or set to false, entities which
// don't conform to one of the pre-defined types will not be surfaced.
	IncludeTypelessEntities *bool

	// A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified.
// If not set (default), or if explicitly set to null, all entities will be
// included.
	MinimumPrecision *float64

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type EntityRecognitionSkill.
func (e *EntityRecognitionSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: e.Context,
		Description: e.Description,
		Inputs: e.Inputs,
		Name: e.Name,
		ODataType: e.ODataType,
		Outputs: e.Outputs,
	}
}

// EntityRecognitionSkillV3 - Using the Text Analytics API, extracts entities of different types from text.
type EntityRecognitionSkillV3 struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// A list of entity categories that should be extracted.
	Categories []*string

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified.
// If not set (default), or if explicitly set to null, all entities will be
// included.
	MinimumPrecision *float64

	// The version of the model to use when calling the Text Analytics API. It will default to the latest available when not specified.
// We recommend you do not specify this value unless absolutely necessary.
	ModelVersion *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type EntityRecognitionSkillV3.
func (e *EntityRecognitionSkillV3) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: e.Context,
		Description: e.Description,
		Inputs: e.Inputs,
		Name: e.Name,
		ODataType: e.ODataType,
		Outputs: e.Outputs,
	}
}

// ErrorAdditionalInfo - The resource management error additional info.
type ErrorAdditionalInfo struct {
	// READ-ONLY; The additional info.
	Info any

	// READ-ONLY; The additional info type.
	Type *string
}

// ErrorDetail - The error detail.
type ErrorDetail struct {
	// READ-ONLY; The error additional info.
	AdditionalInfo []*ErrorAdditionalInfo

	// READ-ONLY; The error code.
	Code *string

	// READ-ONLY; The error details.
	Details []*ErrorDetail

	// READ-ONLY; The error message.
	Message *string

	// READ-ONLY; The error target.
	Target *string
}

// ErrorResponse - Common error response for all Azure Resource Manager APIs to return error details for failed operations.
// (This also follows the OData error response format.).
type ErrorResponse struct {
	// The error object.
	Error *ErrorDetail
}

// ExhaustiveKnnAlgorithmConfiguration - Contains configuration options specific to the exhaustive KNN algorithm used during
// querying, which will perform brute-force search across the entire vector index.
type ExhaustiveKnnAlgorithmConfiguration struct {
	// REQUIRED; The name of the kind of algorithm being configured for use with vector search.
	Kind *VectorSearchAlgorithmKind

	// REQUIRED; The name to associate with this particular configuration.
	Name *string

	// Contains the parameters specific to exhaustive KNN algorithm.
	Parameters *ExhaustiveKnnParameters
}

// GetVectorSearchAlgorithmConfiguration implements the VectorSearchAlgorithmConfigurationClassification interface for type
// ExhaustiveKnnAlgorithmConfiguration.
func (e *ExhaustiveKnnAlgorithmConfiguration) GetVectorSearchAlgorithmConfiguration() *VectorSearchAlgorithmConfiguration {
	return &VectorSearchAlgorithmConfiguration{
		Kind: e.Kind,
		Name: e.Name,
	}
}

// ExhaustiveKnnParameters - Contains the parameters specific to exhaustive KNN algorithm.
type ExhaustiveKnnParameters struct {
	// The similarity metric to use for vector comparisons.
	Metric *VectorSearchAlgorithmMetric
}

// Field - Represents a field in an index definition, which describes the name, data type, and search behavior of a field.
type Field struct {
	// REQUIRED; The name of the field, which must be unique within the fields collection of the index or parent field.
	Name *string

	// REQUIRED; The data type of the field.
	Type *SearchFieldDataType

	// The name of the analyzer to use for the field. This option can be used only with searchable fields and it can't be set
// together with either searchAnalyzer or indexAnalyzer. Once the analyzer is
// chosen, it cannot be changed for the field. Must be null for complex fields.
	Analyzer *LexicalAnalyzerName

	// A value indicating whether to enable the field to be referenced in facet queries. Typically used in a presentation of search
// results that includes hit count by category (for example, search for
// digital cameras and see hits by brand, by megapixels, by price, and so on). This property must be null for complex fields.
// Fields of type Edm.GeographyPoint or Collection(Edm.GeographyPoint) cannot be
// facetable. Default is true for all other simple fields.
	Facetable *bool

	// A list of sub-fields if this is a field of type Edm.ComplexType or Collection(Edm.ComplexType). Must be null or empty for
// simple fields.
	Fields []*Field

	// A value indicating whether to enable the field to be referenced in $filter queries. filterable differs from searchable
// in how strings are handled. Fields of type Edm.String or Collection(Edm.String)
// that are filterable do not undergo word-breaking, so comparisons are for exact matches only. For example, if you set such
// a field f to "sunny day", $filter=f eq 'sunny' will find no matches, but
// $filter=f eq 'sunny day' will. This property must be null for complex fields. Default is true for simple fields and null
// for complex fields.
	Filterable *bool

	// The name of the analyzer used at indexing time for the field. This option can be used only with searchable fields. It must
// be set together with searchAnalyzer and it cannot be set together with the
// analyzer option. This property cannot be set to the name of a language analyzer; use the analyzer property instead if you
// need a language analyzer. Once the analyzer is chosen, it cannot be changed
// for the field. Must be null for complex fields.
	IndexAnalyzer *LexicalAnalyzerName

	// A value indicating whether the field uniquely identifies documents in the index. Exactly one top-level field in each index
// must be chosen as the key field and it must be of type Edm.String. Key fields
// can be used to look up documents directly and update or delete specific documents. Default is false for simple fields and
// null for complex fields.
	Key *bool

	// A value indicating whether the field can be returned in a search result. You can disable this option if you want to use
// a field (for example, margin) as a filter, sorting, or scoring mechanism but do
// not want the field to be visible to the end user. This property must be true for key fields, and it must be null for complex
// fields. This property can be changed on existing fields. Enabling this
// property does not cause any increase in index storage requirements. Default is true for simple fields, false for vector
// fields, and null for complex fields.
	Retrievable *bool

	// The name of the analyzer used at search time for the field. This option can be used only with searchable fields. It must
// be set together with indexAnalyzer and it cannot be set together with the
// analyzer option. This property cannot be set to the name of a language analyzer; use the analyzer property instead if you
// need a language analyzer. This analyzer can be updated on an existing field.
// Must be null for complex fields.
	SearchAnalyzer *LexicalAnalyzerName

	// A value indicating whether the field is full-text searchable. This means it will undergo analysis such as word-breaking
// during indexing. If you set a searchable field to a value like "sunny day",
// internally it will be split into the individual tokens "sunny" and "day". This enables full-text searches for these terms.
// Fields of type Edm.String or Collection(Edm.String) are searchable by
// default. This property must be false for simple fields of other non-string data types, and it must be null for complex
// fields. Note: searchable fields consume extra space in your index to accommodate
// additional tokenized versions of the field value for full-text searches. If you want to save space in your index and you
// don't need a field to be included in searches, set searchable to false.
	Searchable *bool

	// A value indicating whether to enable the field to be referenced in $orderby expressions. By default, the search engine
// sorts results by score, but in many experiences users will want to sort by fields
// in the documents. A simple field can be sortable only if it is single-valued (it has a single value in the scope of the
// parent document). Simple collection fields cannot be sortable, since they are
// multi-valued. Simple sub-fields of complex collections are also multi-valued, and therefore cannot be sortable. This is
// true whether it's an immediate parent field, or an ancestor field, that's the
// complex collection. Complex fields cannot be sortable and the sortable property must be null for such fields. The default
// for sortable is true for single-valued simple fields, false for multi-valued
// simple fields, and null for complex fields.
	Sortable *bool

	// An immutable value indicating whether the field will be persisted separately on disk to be returned in a search result.
// You can disable this option if you don't plan to return the field contents in a
// search response to save on storage overhead. This can only be set during index creation and only for vector fields. This
// property cannot be changed for existing fields or set as false for new fields.
// If this property is set as false, the property 'retrievable' must also be set to false. This property must be true or unset
// for key fields, for new fields, and for non-vector fields, and it must be
// null for complex fields. Disabling this property will reduce index storage requirements. The default is true for vector
// fields.
	Stored *bool

	// A list of the names of synonym maps to associate with this field. This option can be used only with searchable fields.
// Currently only one synonym map per field is supported. Assigning a synonym map to
// a field ensures that query terms targeting that field are expanded at query-time using the rules in the synonym map. This
// attribute can be changed on existing fields. Must be null or an empty
// collection for complex fields.
	SynonymMaps []*string

	// The encoding format to interpret the field contents.
	VectorEncodingFormat *VectorEncodingFormat

	// The dimensionality of the vector field.
	VectorSearchDimensions *int32

	// The name of the vector search profile that specifies the algorithm and vectorizer to use when searching the vector field.
	VectorSearchProfileName *string
}

// FieldMapping - Defines a mapping between a field in a data source and a target field in an index.
type FieldMapping struct {
	// REQUIRED; The name of the field in the data source.
	SourceFieldName *string

	// A function to apply to each source field value before indexing.
	MappingFunction *FieldMappingFunction

	// The name of the target field in the index. Same as the source field name by default.
	TargetFieldName *string
}

// FieldMappingFunction - Represents a function that transforms a value from a data source before indexing.
type FieldMappingFunction struct {
	// REQUIRED; The name of the field mapping function.
	Name *string

	// A dictionary of parameter name/value pairs to pass to the function. Each value must be of a primitive type.
	Parameters map[string]any
}

// FreshnessScoringFunction - Defines a function that boosts scores based on the value of a date-time field.
type FreshnessScoringFunction struct {
	// REQUIRED; A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64

	// REQUIRED; The name of the field used as input to the scoring function.
	FieldName *string

	// REQUIRED; Parameter values for the freshness scoring function.
	Parameters *FreshnessScoringParameters

	// REQUIRED; Indicates the type of function to use. Valid values include magnitude, freshness, distance, and tag. The function
// type must be lower case.
	Type *string

	// A value indicating how boosting will be interpolated across document scores; defaults to "Linear".
	Interpolation *ScoringFunctionInterpolation
}

// GetScoringFunction implements the ScoringFunctionClassification interface for type FreshnessScoringFunction.
func (f *FreshnessScoringFunction) GetScoringFunction() *ScoringFunction {
	return &ScoringFunction{
		Boost: f.Boost,
		FieldName: f.FieldName,
		Interpolation: f.Interpolation,
		Type: f.Type,
	}
}

// FreshnessScoringParameters - Provides parameter values to a freshness scoring function.
type FreshnessScoringParameters struct {
	// REQUIRED; The expiration period after which boosting will stop for a particular document.
	BoostingDuration *string
}

// GetIndexStatisticsResult - Statistics for a given index. Statistics are collected periodically and are not guaranteed to
// always be up-to-date.
type GetIndexStatisticsResult struct {
	// READ-ONLY; The number of documents in the index.
	DocumentCount *int64

	// READ-ONLY; The amount of storage in bytes consumed by the index.
	StorageSize *int64

	// READ-ONLY; The amount of memory in bytes consumed by vectors in the index.
	VectorIndexSize *int64
}

// HighWaterMarkChangeDetectionPolicy - Defines a data change detection policy that captures changes based on the value of
// a high water mark column.
type HighWaterMarkChangeDetectionPolicy struct {
	// REQUIRED; The name of the high water mark column.
	HighWaterMarkColumnName *string

	// REQUIRED; A URI fragment specifying the type of data change detection policy.
	ODataType *string
}

// GetDataChangeDetectionPolicy implements the DataChangeDetectionPolicyClassification interface for type HighWaterMarkChangeDetectionPolicy.
func (h *HighWaterMarkChangeDetectionPolicy) GetDataChangeDetectionPolicy() *DataChangeDetectionPolicy {
	return &DataChangeDetectionPolicy{
		ODataType: h.ODataType,
	}
}

// HnswAlgorithmConfiguration - Contains configuration options specific to the HNSW approximate nearest neighbors algorithm
// used during indexing and querying. The HNSW algorithm offers a tunable trade-off between search speed and
// accuracy.
type HnswAlgorithmConfiguration struct {
	// REQUIRED; The name of the kind of algorithm being configured for use with vector search.
	Kind *VectorSearchAlgorithmKind

	// REQUIRED; The name to associate with this particular configuration.
	Name *string

	// Contains the parameters specific to HNSW algorithm.
	Parameters *HnswParameters
}

// GetVectorSearchAlgorithmConfiguration implements the VectorSearchAlgorithmConfigurationClassification interface for type
// HnswAlgorithmConfiguration.
func (h *HnswAlgorithmConfiguration) GetVectorSearchAlgorithmConfiguration() *VectorSearchAlgorithmConfiguration {
	return &VectorSearchAlgorithmConfiguration{
		Kind: h.Kind,
		Name: h.Name,
	}
}

// HnswParameters - Contains the parameters specific to the HNSW algorithm.
type HnswParameters struct {
	// The size of the dynamic list containing the nearest neighbors, which is used during index time. Increasing this parameter
// may improve index quality, at the expense of increased indexing time. At a
// certain point, increasing this parameter leads to diminishing returns.
	EfConstruction *int32

	// The size of the dynamic list containing the nearest neighbors, which is used during search time. Increasing this parameter
// may improve search results, at the expense of slower search. At a certain
// point, increasing this parameter leads to diminishing returns.
	EfSearch *int32

	// The number of bi-directional links created for every new element during construction. Increasing this parameter value may
// improve recall and reduce retrieval times for datasets with high intrinsic
// dimensionality at the expense of increased memory consumption and longer indexing time.
	M *int32

	// The similarity metric to use for vector comparisons.
	Metric *VectorSearchAlgorithmMetric
}

// ImageAnalysisSkill - A skill that analyzes image files. It extracts a rich set of visual features based on the image content.
type ImageAnalysisSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *ImageAnalysisSkillLanguage

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// A string indicating which domain-specific details to return.
	Details []*ImageDetail

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string

	// A list of visual features.
	VisualFeatures []*VisualFeature
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type ImageAnalysisSkill.
func (i *ImageAnalysisSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: i.Context,
		Description: i.Description,
		Inputs: i.Inputs,
		Name: i.Name,
		ODataType: i.ODataType,
		Outputs: i.Outputs,
	}
}

// Index - Represents a search index definition, which describes the fields and search behavior of an index.
type Index struct {
	// REQUIRED; The fields of the index.
	Fields []*Field

	// REQUIRED; The name of the index.
	Name *string

	// The analyzers for the index.
	Analyzers []LexicalAnalyzerClassification

	// The character filters for the index.
	CharFilters []CharFilterClassification

	// Options to control Cross-Origin Resource Sharing (CORS) for the index.
	CorsOptions *CorsOptions

	// The name of the scoring profile to use if none is specified in the query. If this property is not set and no scoring profile
// is specified in the query, then default scoring (tf-idf) will be used.
	DefaultScoringProfile *string

	// The ETag of the index.
	ETag *string

	// A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level
// of encryption-at-rest for your data when you want full assurance that no one, not
// even Microsoft, can decrypt your data. Once you have encrypted your data, it will always remain encrypted. The search service
// will ignore attempts to set this property to null. You can change this
// property as needed if you want to rotate your encryption key; Your data will be unaffected. Encryption with customer-managed
// keys is not available for free search services, and is only available for
// paid services created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey

	// The scoring profiles for the index.
	ScoringProfiles []*ScoringProfile

	// Defines parameters for a search index that influence semantic capabilities.
	SemanticSearch *SemanticSearch

	// The type of similarity algorithm to be used when scoring and ranking the documents matching a search query. The similarity
// algorithm can only be defined at index creation time and cannot be modified
// on existing indexes. If null, the ClassicSimilarity algorithm is used.
	Similarity SimilarityClassification

	// The suggesters for the index.
	Suggesters []*Suggester

	// The token filters for the index.
	TokenFilters []TokenFilterClassification

	// The tokenizers for the index.
	Tokenizers []LexicalTokenizerClassification

	// Contains configuration options related to vector search.
	VectorSearch *VectorSearch
}

// Indexer - Represents an indexer.
type Indexer struct {
	// REQUIRED; The name of the datasource from which this indexer reads data.
	DataSourceName *string

	// REQUIRED; The name of the indexer.
	Name *string

	// REQUIRED; The name of the index to which this indexer writes data.
	TargetIndexName *string

	// The description of the indexer.
	Description *string

	// The ETag of the indexer.
	ETag *string

	// A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level
// of encryption-at-rest for your indexer definition (as well as indexer execution
// status) when you want full assurance that no one, not even Microsoft, can decrypt them. Once you have encrypted your indexer
// definition, it will always remain encrypted. The search service will ignore
// attempts to set this property to null. You can change this property as needed if you want to rotate your encryption key;
// Your indexer definition (and indexer execution status) will be unaffected.
// Encryption with customer-managed keys is not available for free search services, and is only available for paid services
// created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey

	// Defines mappings between fields in the data source and corresponding target fields in the index.
	FieldMappings []*FieldMapping

	// A value indicating whether the indexer is disabled. Default is false.
	IsDisabled *bool

	// Output field mappings are applied after enrichment and immediately before indexing.
	OutputFieldMappings []*FieldMapping

	// Parameters for indexer execution.
	Parameters *IndexingParameters

	// The schedule for this indexer.
	Schedule *IndexingSchedule

	// The name of the skillset executing with this indexer.
	SkillsetName *string
}

// IndexerDataContainer - Represents information about the entity (such as Azure SQL table or CosmosDB collection) that will
// be indexed.
type IndexerDataContainer struct {
	// REQUIRED; The name of the table or view (for Azure SQL data source) or collection (for CosmosDB data source) that will
// be indexed.
	Name *string

	// A query that is applied to this data container. The syntax and meaning of this parameter is datasource-specific. Not supported
// by Azure SQL datasources.
	Query *string
}

// IndexerDataIdentity - Abstract base type for data identities.
type IndexerDataIdentity struct {
	// REQUIRED; A URI fragment specifying the type of identity.
	ODataType *string
}

// GetIndexerDataIdentity implements the IndexerDataIdentityClassification interface for type IndexerDataIdentity.
func (i *IndexerDataIdentity) GetIndexerDataIdentity() *IndexerDataIdentity { return i }

// IndexerDataNoneIdentity - Clears the identity property of a datasource.
type IndexerDataNoneIdentity struct {
	// REQUIRED; A URI fragment specifying the type of identity.
	ODataType *string
}

// GetIndexerDataIdentity implements the IndexerDataIdentityClassification interface for type IndexerDataNoneIdentity.
func (i *IndexerDataNoneIdentity) GetIndexerDataIdentity() *IndexerDataIdentity {
	return &IndexerDataIdentity{
		ODataType: i.ODataType,
	}
}

// IndexerDataSource - Represents a datasource definition, which can be used to configure an indexer.
type IndexerDataSource struct {
	// REQUIRED; The data container for the datasource.
	Container *IndexerDataContainer

	// REQUIRED; Credentials for the datasource.
	Credentials *DataSourceCredentials

	// REQUIRED; The name of the datasource.
	Name *string

	// REQUIRED; The type of the datasource.
	Type *SearchIndexerDataSourceType

	// The data change detection policy for the datasource.
	DataChangeDetectionPolicy DataChangeDetectionPolicyClassification

	// The data deletion detection policy for the datasource.
	DataDeletionDetectionPolicy DataDeletionDetectionPolicyClassification

	// The description of the datasource.
	Description *string

	// The ETag of the data source.
	ETag *string

	// A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level
// of encryption-at-rest for your datasource definition when you want full assurance
// that no one, not even Microsoft, can decrypt your data source definition. Once you have encrypted your data source definition,
// it will always remain encrypted. The search service will ignore attempts
// to set this property to null. You can change this property as needed if you want to rotate your encryption key; Your datasource
// definition will be unaffected. Encryption with customer-managed keys is
// not available for free search services, and is only available for paid services created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey
}

// IndexerDataUserAssignedIdentity - Specifies the identity for a datasource to use.
type IndexerDataUserAssignedIdentity struct {
	// REQUIRED; A URI fragment specifying the type of identity.
	ODataType *string

	// REQUIRED; The fully qualified Azure resource Id of a user assigned managed identity typically in the form
// "/subscriptions/12345678-1234-1234-1234-1234567890ab/resourceGroups/rg/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myId"
// that should have been assigned to the search service.
	UserAssignedIdentity *string
}

// GetIndexerDataIdentity implements the IndexerDataIdentityClassification interface for type IndexerDataUserAssignedIdentity.
func (i *IndexerDataUserAssignedIdentity) GetIndexerDataIdentity() *IndexerDataIdentity {
	return &IndexerDataIdentity{
		ODataType: i.ODataType,
	}
}

// IndexerError - Represents an item- or document-level indexing error.
type IndexerError struct {
	// READ-ONLY; The message describing the error that occurred while processing the item.
	ErrorMessage *string

	// READ-ONLY; The status code indicating why the indexing operation failed. Possible values include: 400 for a malformed input
// document, 404 for document not found, 409 for a version conflict, 422 when the index is
// temporarily unavailable, or 503 for when the service is too busy.
	StatusCode *int32

	// READ-ONLY; Additional, verbose details about the error to assist in debugging the indexer. This may not be always available.
	Details *string

	// READ-ONLY; A link to a troubleshooting guide for these classes of errors. This may not be always available.
	DocumentationLink *string

	// READ-ONLY; The key of the item for which indexing failed.
	Key *string

	// READ-ONLY; The name of the source at which the error originated. For example, this could refer to a particular skill in
// the attached skillset. This may not be always available.
	Name *string
}

// IndexerExecutionResult - Represents the result of an individual indexer execution.
type IndexerExecutionResult struct {
	// READ-ONLY; The item-level indexing errors.
	Errors []*IndexerError

	// READ-ONLY; The number of items that failed to be indexed during this indexer execution.
	FailedItemCount *int32

	// READ-ONLY; The number of items that were processed during this indexer execution. This includes both successfully processed
// items and items where indexing was attempted but failed.
	ItemCount *int32

	// READ-ONLY; The outcome of this indexer execution.
	Status *IndexerExecutionStatus

	// READ-ONLY; The item-level indexing warnings.
	Warnings []*IndexerWarning

	// READ-ONLY; The end time of this indexer execution, if the execution has already completed.
	EndTime *time.Time

	// READ-ONLY; The error message indicating the top-level error, if any.
	ErrorMessage *string

	// READ-ONLY; Change tracking state with which an indexer execution finished.
	FinalTrackingState *string

	// READ-ONLY; Change tracking state with which an indexer execution started.
	InitialTrackingState *string

	// READ-ONLY; The start time of this indexer execution.
	StartTime *time.Time
}

// IndexerIndexProjectionSelector - Description for what data to store in the designated search index.
type IndexerIndexProjectionSelector struct {
	// REQUIRED; Mappings for the projection, or which source should be mapped to which field in the target index.
	Mappings []*InputFieldMappingEntry

	// REQUIRED; Name of the field in the search index to map the parent document's key value to. Must be a string field that
// is filterable and not the key field.
	ParentKeyFieldName *string

	// REQUIRED; Source context for the projections. Represents the cardinality at which the document will be split into multiple
// sub documents.
	SourceContext *string

	// REQUIRED; Name of the search index to project to. Must have a key field with the 'keyword' analyzer set.
	TargetIndexName *string
}

// IndexerIndexProjections - Definition of additional projections to secondary search indexes.
type IndexerIndexProjections struct {
	// REQUIRED; A list of projections to be performed to secondary search indexes.
	Selectors []*IndexerIndexProjectionSelector

	// A dictionary of index projection-specific configuration properties. Each name is the name of a specific property. Each
// value must be of a primitive type.
	Parameters *IndexerIndexProjectionsParameters
}

// IndexerIndexProjectionsParameters - A dictionary of index projection-specific configuration properties. Each name is the
// name of a specific property. Each value must be of a primitive type.
type IndexerIndexProjectionsParameters struct {
	// OPTIONAL; Contains additional key/value pairs not defined in the schema.
	AdditionalProperties map[string]any

	// Defines behavior of the index projections in relation to the rest of the indexer.
	ProjectionMode *IndexProjectionMode
}

// IndexerKnowledgeStore - Definition of additional projections to azure blob, table, or files, of enriched data.
type IndexerKnowledgeStore struct {
	// REQUIRED; A list of additional projections to perform during indexing.
	Projections []*IndexerKnowledgeStoreProjection

	// REQUIRED; The connection string to the storage account projections will be stored in.
	StorageConnectionString *string
}

// IndexerKnowledgeStoreBlobProjectionSelector - Abstract class to share properties between concrete selectors.
type IndexerKnowledgeStoreBlobProjectionSelector struct {
	// REQUIRED; Blob container to store projections in.
	StorageContainer *string

	// Name of generated key to store projection under.
	GeneratedKeyName *string

	// Nested inputs for complex projections.
	Inputs []*InputFieldMappingEntry

	// Name of reference key to different projection.
	ReferenceKeyName *string

	// Source data to project.
	Source *string

	// Source context for complex projections.
	SourceContext *string
}

// IndexerKnowledgeStoreFileProjectionSelector - Projection definition for what data to store in Azure Files.
type IndexerKnowledgeStoreFileProjectionSelector struct {
	// REQUIRED; Blob container to store projections in.
	StorageContainer *string

	// Name of generated key to store projection under.
	GeneratedKeyName *string

	// Nested inputs for complex projections.
	Inputs []*InputFieldMappingEntry

	// Name of reference key to different projection.
	ReferenceKeyName *string

	// Source data to project.
	Source *string

	// Source context for complex projections.
	SourceContext *string
}

// IndexerKnowledgeStoreObjectProjectionSelector - Projection definition for what data to store in Azure Blob.
type IndexerKnowledgeStoreObjectProjectionSelector struct {
	// REQUIRED; Blob container to store projections in.
	StorageContainer *string

	// Name of generated key to store projection under.
	GeneratedKeyName *string

	// Nested inputs for complex projections.
	Inputs []*InputFieldMappingEntry

	// Name of reference key to different projection.
	ReferenceKeyName *string

	// Source data to project.
	Source *string

	// Source context for complex projections.
	SourceContext *string
}

// IndexerKnowledgeStoreParameters - A dictionary of knowledge store-specific configuration properties. Each name is the name
// of a specific property. Each value must be of a primitive type.
type IndexerKnowledgeStoreParameters struct {
	// OPTIONAL; Contains additional key/value pairs not defined in the schema.
	AdditionalProperties map[string]any

	// Whether or not projections should synthesize a generated key name if one isn't already present.
	SynthesizeGeneratedKeyName *bool
}

// IndexerKnowledgeStoreProjection - Container object for various projection selectors.
type IndexerKnowledgeStoreProjection struct {
	// Projections to Azure File storage.
	Files []*IndexerKnowledgeStoreFileProjectionSelector

	// Projections to Azure Blob storage.
	Objects []*IndexerKnowledgeStoreObjectProjectionSelector

	// Projections to Azure Table storage.
	Tables []*IndexerKnowledgeStoreTableProjectionSelector
}

// IndexerKnowledgeStoreProjectionSelector - Abstract class to share properties between concrete selectors.
type IndexerKnowledgeStoreProjectionSelector struct {
	// Name of generated key to store projection under.
	GeneratedKeyName *string

	// Nested inputs for complex projections.
	Inputs []*InputFieldMappingEntry

	// Name of reference key to different projection.
	ReferenceKeyName *string

	// Source data to project.
	Source *string

	// Source context for complex projections.
	SourceContext *string
}

// IndexerKnowledgeStoreTableProjectionSelector - Description for what data to store in Azure Tables.
type IndexerKnowledgeStoreTableProjectionSelector struct {
	// REQUIRED; Name of the Azure table to store projected data in.
	TableName *string

	// Name of generated key to store projection under.
	GeneratedKeyName *string

	// Nested inputs for complex projections.
	Inputs []*InputFieldMappingEntry

	// Name of reference key to different projection.
	ReferenceKeyName *string

	// Source data to project.
	Source *string

	// Source context for complex projections.
	SourceContext *string
}

type IndexerLimits struct {
	// READ-ONLY; The maximum number of characters that will be extracted from a document picked up for indexing.
	MaxDocumentContentCharactersToExtract *int64

	// READ-ONLY; The maximum size of a document, in bytes, which will be considered valid for indexing.
	MaxDocumentExtractionSize *int64

	// READ-ONLY; The maximum duration that the indexer is permitted to run for one execution.
	MaxRunTime *string
}

// IndexerSkill - Base type for skills.
type IndexerSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type IndexerSkill.
func (i *IndexerSkill) GetIndexerSkill() *IndexerSkill { return i }

// IndexerSkillset - A list of skills.
type IndexerSkillset struct {
	// REQUIRED; The name of the skillset.
	Name *string

	// REQUIRED; A list of skills in the skillset.
	Skills []IndexerSkillClassification

	// Details about the Azure AI service to be used when running skills.
	CognitiveServicesAccount CognitiveServicesAccountClassification

	// The description of the skillset.
	Description *string

	// The ETag of the skillset.
	ETag *string

	// A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level
// of encryption-at-rest for your skillset definition when you want full assurance
// that no one, not even Microsoft, can decrypt your skillset definition. Once you have encrypted your skillset definition,
// it will always remain encrypted. The search service will ignore attempts to set
// this property to null. You can change this property as needed if you want to rotate your encryption key; Your skillset
// definition will be unaffected. Encryption with customer-managed keys is not
// available for free search services, and is only available for paid services created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey

	// Definition of additional projections to secondary search index(es).
	IndexProjections *IndexerIndexProjections

	// Definition of additional projections to Azure blob, table, or files, of enriched data.
	KnowledgeStore *IndexerKnowledgeStore
}

// IndexerStatus2 - Represents the current status and execution history of an indexer.
type IndexerStatus2 struct {
	// READ-ONLY; History of the recent indexer executions, sorted in reverse chronological order.
	ExecutionHistory []*IndexerExecutionResult

	// READ-ONLY; The execution limits for the indexer.
	Limits *IndexerLimits

	// READ-ONLY; Overall indexer status.
	Status *IndexerStatus

	// READ-ONLY; The result of the most recent or an in-progress indexer execution.
	LastResult *IndexerExecutionResult
}

// IndexerWarning - Represents an item-level warning.
type IndexerWarning struct {
	// READ-ONLY; The message describing the warning that occurred while processing the item.
	Message *string

	// READ-ONLY; Additional, verbose details about the warning to assist in debugging the indexer. This may not be always available.
	Details *string

	// READ-ONLY; A link to a troubleshooting guide for these classes of warnings. This may not be always available.
	DocumentationLink *string

	// READ-ONLY; The key of the item which generated a warning.
	Key *string

	// READ-ONLY; The name of the source at which the warning originated. For example, this could refer to a particular skill
// in the attached skillset. This may not be always available.
	Name *string
}

// IndexingParameters - Represents parameters for indexer execution.
type IndexingParameters struct {
	// The number of items that are read from the data source and indexed as a single batch in order to improve performance. The
// default depends on the data source type.
	BatchSize *int32

	// A dictionary of indexer-specific configuration properties. Each name is the name of a specific property. Each value must
// be of a primitive type.
	Configuration *IndexingParametersConfiguration

	// The maximum number of items that can fail indexing for indexer execution to still be considered successful. -1 means no
// limit. Default is 0.
	MaxFailedItems *int32

	// The maximum number of items in a single batch that can fail indexing for the batch to still be considered successful. -1
// means no limit. Default is 0.
	MaxFailedItemsPerBatch *int32
}

// IndexingParametersConfiguration - A dictionary of indexer-specific configuration properties. Each name is the name of a
// specific property. Each value must be of a primitive type.
type IndexingParametersConfiguration struct {
	// OPTIONAL; Contains additional key/value pairs not defined in the schema.
	AdditionalProperties map[string]any

	// If true, will create a path //document//file_data that is an object representing the original file data downloaded from
// your blob data source. This allows you to pass the original file data to a
// custom skill for processing within the enrichment pipeline, or to the Document Extraction skill.
	AllowSkillsetToReadFileData *bool

	// Specifies the data to extract from Azure blob storage and tells the indexer which data to extract from image content when
// "imageAction" is set to a value other than "none". This applies to embedded
// image content in a .PDF or other application, or image files such as .jpg and .png, in Azure blobs.
	DataToExtract *BlobIndexerDataToExtract

	// For CSV blobs, specifies the end-of-line single-character delimiter for CSV files where each line starts a new document
// (for example, "|").
	DelimitedTextDelimiter *string

	// For CSV blobs, specifies a comma-delimited list of column headers, useful for mapping source fields to destination fields
// in an index.
	DelimitedTextHeaders *string

	// For JSON arrays, given a structured or semi-structured document, you can specify a path to the array using this property.
	DocumentRoot *string

	// Comma-delimited list of filename extensions to ignore when processing from Azure blob storage. For example, you could exclude
// ".png, .mp4" to skip over those files during indexing.
	ExcludedFileNameExtensions *string

	// Specifies the environment in which the indexer should execute.
	ExecutionEnvironment *IndexerExecutionEnvironment

	// For Azure blobs, set to false if you want to continue indexing if a document fails indexing.
	FailOnUnprocessableDocument *bool

	// For Azure blobs, set to false if you want to continue indexing when an unsupported content type is encountered, and you
// don't know all the content types (file extensions) in advance.
	FailOnUnsupportedContentType *bool

	// For CSV blobs, indicates that the first (non-blank) line of each blob contains headers.
	FirstLineContainsHeaders *bool

	// Determines how to process embedded images and image files in Azure blob storage. Setting the "imageAction" configuration
// to any value other than "none" requires that a skillset also be attached to
// that indexer.
	ImageAction *BlobIndexerImageAction

	// For Azure blobs, set this property to true to still index storage metadata for blob content that is too large to process.
// Oversized blobs are treated as errors by default. For limits on blob size, see
// https://learn.microsoft.com/azure/search/search-limits-quotas-capacity.
	IndexStorageMetadataOnlyForOversizedDocuments *bool

	// Comma-delimited list of filename extensions to select when processing from Azure blob storage. For example, you could focus
// indexing on specific application files ".docx, .pptx, .msg" to specifically
// include those file types.
	IndexedFileNameExtensions *string

	// Determines algorithm for text extraction from PDF files in Azure blob storage.
	PDFTextRotationAlgorithm *BlobIndexerPDFTextRotationAlgorithm

	// Represents the parsing mode for indexing from an Azure blob data source.
	ParsingMode *BlobIndexerParsingMode

	// Increases the timeout beyond the 5-minute default for Azure SQL database data sources, specified in the format "hh:mm:ss".
	QueryTimeout *string
}

// IndexingSchedule - Represents a schedule for indexer execution.
type IndexingSchedule struct {
	// REQUIRED; The interval of time between indexer executions.
	Interval *string

	// The time when an indexer should start running.
	StartTime *time.Time
}

// InputFieldMappingEntry - Input field mapping for a skill.
type InputFieldMappingEntry struct {
	// REQUIRED; The name of the input.
	Name *string

	// The recursive inputs used when creating a complex type.
	Inputs []*InputFieldMappingEntry

	// The source of the input.
	Source *string

	// The source context used for selecting recursive inputs.
	SourceContext *string
}

// KeepTokenFilter - A token filter that only keeps tokens with text contained in a specified list of words. This token filter
// is implemented using Apache Lucene.
type KeepTokenFilter struct {
	// REQUIRED; The list of words to keep.
	KeepWords []*string

	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// A value indicating whether to lower case all words first. Default is false.
	LowerCaseKeepWords *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type KeepTokenFilter.
func (k *KeepTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: k.Name,
		ODataType: k.ODataType,
	}
}

// KeyPhraseExtractionSkill - A skill that uses text analytics for key phrase extraction.
type KeyPhraseExtractionSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *KeyPhraseExtractionSkillLanguage

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// A number indicating how many key phrases to return. If absent, all identified key phrases will be returned.
	MaxKeyPhraseCount *int32

	// The version of the model to use when calling the Text Analytics service. It will default to the latest available when not
// specified. We recommend you do not specify this value unless absolutely
// necessary.
	ModelVersion *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type KeyPhraseExtractionSkill.
func (k *KeyPhraseExtractionSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: k.Context,
		Description: k.Description,
		Inputs: k.Inputs,
		Name: k.Name,
		ODataType: k.ODataType,
		Outputs: k.Outputs,
	}
}

// KeywordMarkerTokenFilter - Marks terms as keywords. This token filter is implemented using Apache Lucene.
type KeywordMarkerTokenFilter struct {
	// REQUIRED; A list of words to mark as keywords.
	Keywords []*string

	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// A value indicating whether to ignore case. If true, all words are converted to lower case first. Default is false.
	IgnoreCase *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type KeywordMarkerTokenFilter.
func (k *KeywordMarkerTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: k.Name,
		ODataType: k.ODataType,
	}
}

// KeywordTokenizer - Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.
type KeywordTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// The read buffer size in bytes. Default is 256.
	BufferSize *int32
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type KeywordTokenizer.
func (k *KeywordTokenizer) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: k.Name,
		ODataType: k.ODataType,
	}
}

// KeywordTokenizerV2 - Emits the entire input as a single token. This tokenizer is implemented using Apache Lucene.
type KeywordTokenizerV2 struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// The maximum token length. Default is 256. Tokens longer than the maximum length are split. The maximum token length that
// can be used is 300 characters.
	MaxTokenLength *int32
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type KeywordTokenizerV2.
func (k *KeywordTokenizerV2) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: k.Name,
		ODataType: k.ODataType,
	}
}

// LanguageDetectionSkill - A skill that detects the language of input text and reports a single language code for every document
// submitted on the request. The language code is paired with a score indicating the confidence of
// the analysis.
type LanguageDetectionSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A country code to use as a hint to the language detection model if it cannot disambiguate the language.
	DefaultCountryHint *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The version of the model to use when calling the Text Analytics service. It will default to the latest available when not
// specified. We recommend you do not specify this value unless absolutely
// necessary.
	ModelVersion *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type LanguageDetectionSkill.
func (l *LanguageDetectionSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: l.Context,
		Description: l.Description,
		Inputs: l.Inputs,
		Name: l.Name,
		ODataType: l.ODataType,
		Outputs: l.Outputs,
	}
}

// LengthTokenFilter - Removes words that are too long or too short. This token filter is implemented using Apache Lucene.
type LengthTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The maximum length in characters. Default and maximum is 300.
	MaxLength *int32

	// The minimum length in characters. Default is 0. Maximum is 300. Must be less than the value of max.
	MinLength *int32
}

// GetTokenFilter implements the TokenFilterClassification interface for type LengthTokenFilter.
func (l *LengthTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: l.Name,
		ODataType: l.ODataType,
	}
}

// LexicalAnalyzer - Base type for analyzers.
type LexicalAnalyzer struct {
	// REQUIRED; The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of analyzer.
	ODataType *string
}

// GetLexicalAnalyzer implements the LexicalAnalyzerClassification interface for type LexicalAnalyzer.
func (l *LexicalAnalyzer) GetLexicalAnalyzer() *LexicalAnalyzer { return l }

// LexicalTokenizer - Base type for tokenizers.
type LexicalTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type LexicalTokenizer.
func (l *LexicalTokenizer) GetLexicalTokenizer() *LexicalTokenizer { return l }

// LimitTokenFilter - Limits the number of tokens while indexing. This token filter is implemented using Apache Lucene.
type LimitTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// A value indicating whether all tokens from the input must be consumed even if maxTokenCount is reached. Default is false.
	ConsumeAllTokens *bool

	// The maximum number of tokens to produce. Default is 1.
	MaxTokenCount *int32
}

// GetTokenFilter implements the TokenFilterClassification interface for type LimitTokenFilter.
func (l *LimitTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: l.Name,
		ODataType: l.ODataType,
	}
}

// ListDataSourcesResult - Response from a List Datasources request. If successful, it includes the full definitions of all
// datasources.
type ListDataSourcesResult struct {
	// READ-ONLY; The datasources in the Search service.
	DataSources []*IndexerDataSource
}

// ListIndexersResult - Response from a List Indexers request. If successful, it includes the full definitions of all indexers.
type ListIndexersResult struct {
	// READ-ONLY; The indexers in the Search service.
	Indexers []*Indexer
}

// ListIndexesResult - Response from a List Indexes request. If successful, it includes the full definitions of all indexes.
type ListIndexesResult struct {
	// READ-ONLY; The indexes in the Search service.
	Indexes []*Index
}

// ListSkillsetsResult - Response from a list skillset request. If successful, it includes the full definitions of all skillsets.
type ListSkillsetsResult struct {
	// READ-ONLY; The skillsets defined in the Search service.
	Skillsets []*IndexerSkillset
}

// ListSynonymMapsResult - Response from a List SynonymMaps request. If successful, it includes the full definitions of all
// synonym maps.
type ListSynonymMapsResult struct {
	// READ-ONLY; The synonym maps in the Search service.
	SynonymMaps []*SynonymMap
}

// LuceneStandardAnalyzer - Standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase filter and stop
// filter.
type LuceneStandardAnalyzer struct {
	// REQUIRED; The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of analyzer.
	ODataType *string

	// The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that
// can be used is 300 characters.
	MaxTokenLength *int32

	// A list of stopwords.
	Stopwords []*string
}

// GetLexicalAnalyzer implements the LexicalAnalyzerClassification interface for type LuceneStandardAnalyzer.
func (l *LuceneStandardAnalyzer) GetLexicalAnalyzer() *LexicalAnalyzer {
	return &LexicalAnalyzer{
		Name: l.Name,
		ODataType: l.ODataType,
	}
}

// LuceneStandardTokenizer - Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using
// Apache Lucene.
type LuceneStandardTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// The maximum token length. Default is 255. Tokens longer than the maximum length are split.
	MaxTokenLength *int32
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type LuceneStandardTokenizer.
func (l *LuceneStandardTokenizer) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: l.Name,
		ODataType: l.ODataType,
	}
}

// LuceneStandardTokenizerV2 - Breaks text following the Unicode Text Segmentation rules. This tokenizer is implemented using
// Apache Lucene.
type LuceneStandardTokenizerV2 struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that
// can be used is 300 characters.
	MaxTokenLength *int32
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type LuceneStandardTokenizerV2.
func (l *LuceneStandardTokenizerV2) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: l.Name,
		ODataType: l.ODataType,
	}
}

// MagnitudeScoringFunction - Defines a function that boosts scores based on the magnitude of a numeric field.
type MagnitudeScoringFunction struct {
	// REQUIRED; A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64

	// REQUIRED; The name of the field used as input to the scoring function.
	FieldName *string

	// REQUIRED; Parameter values for the magnitude scoring function.
	Parameters *MagnitudeScoringParameters

	// REQUIRED; Indicates the type of function to use. Valid values include magnitude, freshness, distance, and tag. The function
// type must be lower case.
	Type *string

	// A value indicating how boosting will be interpolated across document scores; defaults to "Linear".
	Interpolation *ScoringFunctionInterpolation
}

// GetScoringFunction implements the ScoringFunctionClassification interface for type MagnitudeScoringFunction.
func (m *MagnitudeScoringFunction) GetScoringFunction() *ScoringFunction {
	return &ScoringFunction{
		Boost: m.Boost,
		FieldName: m.FieldName,
		Interpolation: m.Interpolation,
		Type: m.Type,
	}
}

// MagnitudeScoringParameters - Provides parameter values to a magnitude scoring function.
type MagnitudeScoringParameters struct {
	// REQUIRED; The field value at which boosting ends.
	BoostingRangeEnd *float64

	// REQUIRED; The field value at which boosting starts.
	BoostingRangeStart *float64

	// A value indicating whether to apply a constant boost for field values beyond the range end value; default is false.
	ShouldBoostBeyondRangeByConstant *bool
}

// MappingCharFilter - A character filter that applies mappings defined with the mappings option. Matching is greedy (longest
// pattern matching at a given point wins). Replacement is allowed to be the empty string. This
// character filter is implemented using Apache Lucene.
type MappingCharFilter struct {
	// REQUIRED; A list of mappings of the following format: "a=>b" (all occurrences of the character "a" will be replaced with
// character "b").
	Mappings []*string

	// REQUIRED; The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of char filter.
	ODataType *string
}

// GetCharFilter implements the CharFilterClassification interface for type MappingCharFilter.
func (m *MappingCharFilter) GetCharFilter() *CharFilter {
	return &CharFilter{
		Name: m.Name,
		ODataType: m.ODataType,
	}
}

// MergeSkill - A skill for merging two or more strings into a single unified string, with an optional user-defined delimiter
// separating each component part.
type MergeSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The tag indicates the end of the merged text. By default, the tag is an empty space.
	InsertPostTag *string

	// The tag indicates the start of the merged text. By default, the tag is an empty space.
	InsertPreTag *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type MergeSkill.
func (m *MergeSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: m.Context,
		Description: m.Description,
		Inputs: m.Inputs,
		Name: m.Name,
		ODataType: m.ODataType,
		Outputs: m.Outputs,
	}
}

// MicrosoftLanguageStemmingTokenizer - Divides text using language-specific rules and reduces words to their base forms.
type MicrosoftLanguageStemmingTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set to false if used as the
// indexing tokenizer. Default is false.
	IsSearchTokenizer *bool

	// The language to use. The default is English.
	Language *MicrosoftStemmingTokenizerLanguage

	// The maximum token length. Tokens longer than the maximum length are split. Maximum token length that can be used is 300
// characters. Tokens longer than 300 characters are first split into tokens of
// length 300 and then each of those tokens is split based on the max token length set. Default is 255.
	MaxTokenLength *int32
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type MicrosoftLanguageStemmingTokenizer.
func (m *MicrosoftLanguageStemmingTokenizer) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: m.Name,
		ODataType: m.ODataType,
	}
}

// MicrosoftLanguageTokenizer - Divides text using language-specific rules.
type MicrosoftLanguageTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// A value indicating how the tokenizer is used. Set to true if used as the search tokenizer, set to false if used as the
// indexing tokenizer. Default is false.
	IsSearchTokenizer *bool

	// The language to use. The default is English.
	Language *MicrosoftTokenizerLanguage

	// The maximum token length. Tokens longer than the maximum length are split. Maximum token length that can be used is 300
// characters. Tokens longer than 300 characters are first split into tokens of
// length 300 and then each of those tokens is split based on the max token length set. Default is 255.
	MaxTokenLength *int32
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type MicrosoftLanguageTokenizer.
func (m *MicrosoftLanguageTokenizer) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: m.Name,
		ODataType: m.ODataType,
	}
}

// NGramTokenFilter - Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.
type NGramTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The maximum n-gram length. Default is 2.
	MaxGram *int32

	// The minimum n-gram length. Default is 1. Must be less than the value of maxGram.
	MinGram *int32
}

// GetTokenFilter implements the TokenFilterClassification interface for type NGramTokenFilter.
func (n *NGramTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: n.Name,
		ODataType: n.ODataType,
	}
}

// NGramTokenFilterV2 - Generates n-grams of the given size(s). This token filter is implemented using Apache Lucene.
type NGramTokenFilterV2 struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32

	// The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32
}

// GetTokenFilter implements the TokenFilterClassification interface for type NGramTokenFilterV2.
func (n *NGramTokenFilterV2) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: n.Name,
		ODataType: n.ODataType,
	}
}

// NGramTokenizer - Tokenizes the input into n-grams of the given size(s). This tokenizer is implemented using Apache Lucene.
type NGramTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// The maximum n-gram length. Default is 2. Maximum is 300.
	MaxGram *int32

	// The minimum n-gram length. Default is 1. Maximum is 300. Must be less than the value of maxGram.
	MinGram *int32

	// Character classes to keep in the tokens.
	TokenChars []*TokenCharacterKind
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type NGramTokenizer.
func (n *NGramTokenizer) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: n.Name,
		ODataType: n.ODataType,
	}
}

// OcrSkill - A skill that extracts text from image files.
type OcrSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *OcrSkillLanguage

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// Defines the sequence of characters to use between the lines of text recognized by the OCR skill. The default value is "space".
	LineEnding *LineEnding

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string

	// A value indicating to turn orientation detection on or not. Default is false.
	ShouldDetectOrientation *bool
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type OcrSkill.
func (o *OcrSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: o.Context,
		Description: o.Description,
		Inputs: o.Inputs,
		Name: o.Name,
		ODataType: o.ODataType,
		Outputs: o.Outputs,
	}
}

// OutputFieldMappingEntry - Output field mapping for a skill.
type OutputFieldMappingEntry struct {
	// REQUIRED; The name of the output defined by the skill.
	Name *string

	// The target name of the output. It is optional and default to name.
	TargetName *string
}

// PIIDetectionSkill - Using the Text Analytics API, extracts personal information from an input text and gives you the option
// of masking it.
type PIIDetectionSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// If specified, will set the PII domain to include only a subset of the entity categories. Possible values include: 'phi',
// 'none'. Default is 'none'.
	Domain *string

	// The character used to mask the text if the maskingMode parameter is set to replace. Default is '*'.
	Mask *string

	// A parameter that provides various ways to mask the personal information detected in the input text. Default is 'none'.
	MaskingMode *PIIDetectionSkillMaskingMode

	// A value between 0 and 1 that be used to only include entities whose confidence score is greater than the value specified.
// If not set (default), or if explicitly set to null, all entities will be
// included.
	MinimumPrecision *float64

	// The version of the model to use when calling the Text Analytics service. It will default to the latest available when not
// specified. We recommend you do not specify this value unless absolutely
// necessary.
	ModelVersion *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string

	// A list of PII entity categories that should be extracted and masked.
	PiiCategories []*string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type PIIDetectionSkill.
func (p *PIIDetectionSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: p.Context,
		Description: p.Description,
		Inputs: p.Inputs,
		Name: p.Name,
		ODataType: p.ODataType,
		Outputs: p.Outputs,
	}
}

// PathHierarchyTokenizerV2 - Tokenizer for path-like hierarchies. This tokenizer is implemented using Apache Lucene.
type PathHierarchyTokenizerV2 struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// The delimiter character to use. Default is "/".
	Delimiter *rune

	// The maximum token length. Default and maximum is 300.
	MaxTokenLength *int32

	// The number of initial tokens to skip. Default is 0.
	NumberOfTokensToSkip *int32

	// A value that, if set, replaces the delimiter character. Default is "/".
	Replacement *rune

	// A value indicating whether to generate tokens in reverse order. Default is false.
	ReverseTokenOrder *bool
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type PathHierarchyTokenizerV2.
func (p *PathHierarchyTokenizerV2) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: p.Name,
		ODataType: p.ODataType,
	}
}

// PatternAnalyzer - Flexibly separates text into terms via a regular expression pattern. This analyzer is implemented using
// Apache Lucene.
type PatternAnalyzer struct {
	// REQUIRED; The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of analyzer.
	ODataType *string

	// Regular expression flags.
	Flags *RegexFlags

	// A value indicating whether terms should be lower-cased. Default is true.
	LowerCaseTerms *bool

	// A regular expression pattern to match token separators. Default is an expression that matches one or more non-word characters.
	Pattern *string

	// A list of stopwords.
	Stopwords []*string
}

// GetLexicalAnalyzer implements the LexicalAnalyzerClassification interface for type PatternAnalyzer.
func (p *PatternAnalyzer) GetLexicalAnalyzer() *LexicalAnalyzer {
	return &LexicalAnalyzer{
		Name: p.Name,
		ODataType: p.ODataType,
	}
}

// PatternCaptureTokenFilter - Uses Java regexes to emit multiple tokens - one for each capture group in one or more patterns.
// This token filter is implemented using Apache Lucene.
type PatternCaptureTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// REQUIRED; A list of patterns to match against each token.
	Patterns []*string

	// A value indicating whether to return the original token even if one of the patterns matches. Default is true.
	PreserveOriginal *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type PatternCaptureTokenFilter.
func (p *PatternCaptureTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: p.Name,
		ODataType: p.ODataType,
	}
}

// PatternReplaceCharFilter - A character filter that replaces characters in the input string. It uses a regular expression
// to identify character sequences to preserve and a replacement pattern to identify characters to replace.
// For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement "$1#$2", the result would be "aa#bb
// aa#bb". This character filter is implemented using Apache Lucene.
type PatternReplaceCharFilter struct {
	// REQUIRED; The name of the char filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of char filter.
	ODataType *string

	// REQUIRED; A regular expression pattern.
	Pattern *string

	// REQUIRED; The replacement text.
	Replacement *string
}

// GetCharFilter implements the CharFilterClassification interface for type PatternReplaceCharFilter.
func (p *PatternReplaceCharFilter) GetCharFilter() *CharFilter {
	return &CharFilter{
		Name: p.Name,
		ODataType: p.ODataType,
	}
}

// PatternReplaceTokenFilter - A character filter that replaces characters in the input string. It uses a regular expression
// to identify character sequences to preserve and a replacement pattern to identify characters to replace.
// For example, given the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement "$1#$2", the result would be "aa#bb
// aa#bb". This token filter is implemented using Apache Lucene.
type PatternReplaceTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// REQUIRED; A regular expression pattern.
	Pattern *string

	// REQUIRED; The replacement text.
	Replacement *string
}

// GetTokenFilter implements the TokenFilterClassification interface for type PatternReplaceTokenFilter.
func (p *PatternReplaceTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: p.Name,
		ODataType: p.ODataType,
	}
}

// PatternTokenizer - Tokenizer that uses regex pattern matching to construct distinct tokens. This tokenizer is implemented
// using Apache Lucene.
type PatternTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// Regular expression flags.
	Flags *RegexFlags

	// The zero-based ordinal of the matching group in the regular expression pattern to extract into tokens. Use -1 if you want
// to use the entire pattern to split the input into tokens, irrespective of
// matching groups. Default is -1.
	Group *int32

	// A regular expression pattern to match token separators. Default is an expression that matches one or more non-word characters.
	Pattern *string
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type PatternTokenizer.
func (p *PatternTokenizer) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: p.Name,
		ODataType: p.ODataType,
	}
}

// PhoneticTokenFilter - Create tokens for phonetic matches. This token filter is implemented using Apache Lucene.
type PhoneticTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The phonetic encoder to use. Default is "metaphone".
	Encoder *PhoneticEncoder

	// A value indicating whether encoded tokens should replace original tokens. If false, encoded tokens are added as synonyms.
// Default is true.
	ReplaceOriginalTokens *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type PhoneticTokenFilter.
func (p *PhoneticTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: p.Name,
		ODataType: p.ODataType,
	}
}

// ResourceCounter - Represents a resource's usage and quota.
type ResourceCounter struct {
	// REQUIRED; The resource usage amount.
	Usage *int64

	// The resource amount quota.
	Quota *int64
}

// ResourceEncryptionKey - A customer-managed encryption key in Azure Key Vault. Keys that you create and manage can be used
// to encrypt or decrypt data-at-rest, such as indexes and synonym maps.
type ResourceEncryptionKey struct {
	// REQUIRED; The name of your Azure Key Vault key to be used to encrypt your data at rest.
	KeyName *string

	// REQUIRED; The version of your Azure Key Vault key to be used to encrypt your data at rest.
	KeyVersion *string

	// REQUIRED; The URI of your Azure Key Vault, also referred to as DNS name, that contains the key to be used to encrypt your
// data at rest. An example URI might be https://my-keyvault-name.vault.azure.net.
	VaultURI *string

	// Optional Azure Active Directory credentials used for accessing your Azure Key Vault. Not required if using managed identity
// instead.
	AccessCredentials *AzureActiveDirectoryApplicationCredentials
}

// SQLIntegratedChangeTrackingPolicy - Defines a data change detection policy that captures changes using the Integrated Change
// Tracking feature of Azure SQL Database.
type SQLIntegratedChangeTrackingPolicy struct {
	// REQUIRED; A URI fragment specifying the type of data change detection policy.
	ODataType *string
}

// GetDataChangeDetectionPolicy implements the DataChangeDetectionPolicyClassification interface for type SQLIntegratedChangeTrackingPolicy.
func (s *SQLIntegratedChangeTrackingPolicy) GetDataChangeDetectionPolicy() *DataChangeDetectionPolicy {
	return &DataChangeDetectionPolicy{
		ODataType: s.ODataType,
	}
}

// ScalarQuantizationCompressionConfiguration - Contains configuration options specific to the scalar quantization compression
// method used during indexing and querying.
type ScalarQuantizationCompressionConfiguration struct {
	// REQUIRED; The name of the kind of compression method being configured for use with vector search.
	Kind *VectorSearchCompressionKind

	// REQUIRED; The name to associate with this particular configuration.
	Name *string

	// Default oversampling factor. Oversampling will internally request more documents (specified by this multiplier) in the
// initial search. This increases the set of results that will be reranked using
// recomputed similarity scores from full-precision vectors. Minimum value is 1, meaning no oversampling (1x). This parameter
// can only be set when rerankWithOriginalVectors is true. Higher values improve
// recall at the expense of latency.
	DefaultOversampling *float64

	// Contains the parameters specific to Scalar Quantization.
	Parameters *ScalarQuantizationParameters

	// If set to true, once the ordered set of results calculated using compressed vectors are obtained, they will be reranked
// again by recalculating the full-precision similarity scores. This will improve
// recall at the expense of latency.
	RerankWithOriginalVectors *bool
}

// GetVectorSearchCompressionConfiguration implements the VectorSearchCompressionConfigurationClassification interface for
// type ScalarQuantizationCompressionConfiguration.
func (s *ScalarQuantizationCompressionConfiguration) GetVectorSearchCompressionConfiguration() *VectorSearchCompressionConfiguration {
	return &VectorSearchCompressionConfiguration{
		DefaultOversampling: s.DefaultOversampling,
		Kind: s.Kind,
		Name: s.Name,
		RerankWithOriginalVectors: s.RerankWithOriginalVectors,
	}
}

// ScalarQuantizationParameters - Contains the parameters specific to Scalar Quantization.
type ScalarQuantizationParameters struct {
	// The quantized data type of compressed vector values.
	QuantizedDataType *VectorSearchCompressionTargetDataType
}

// ScoringFunction - Base type for functions that can modify document scores during ranking.
type ScoringFunction struct {
	// REQUIRED; A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64

	// REQUIRED; The name of the field used as input to the scoring function.
	FieldName *string

	// REQUIRED; Indicates the type of function to use. Valid values include magnitude, freshness, distance, and tag. The function
// type must be lower case.
	Type *string

	// A value indicating how boosting will be interpolated across document scores; defaults to "Linear".
	Interpolation *ScoringFunctionInterpolation
}

// GetScoringFunction implements the ScoringFunctionClassification interface for type ScoringFunction.
func (s *ScoringFunction) GetScoringFunction() *ScoringFunction { return s }

// ScoringProfile - Defines parameters for a search index that influence scoring in search queries.
type ScoringProfile struct {
	// REQUIRED; The name of the scoring profile.
	Name *string

	// A value indicating how the results of individual scoring functions should be combined. Defaults to "Sum". Ignored if there
// are no scoring functions.
	FunctionAggregation *ScoringFunctionAggregation

	// The collection of functions that influence the scoring of documents.
	Functions []ScoringFunctionClassification

	// Parameters that boost scoring based on text matches in certain index fields.
	TextWeights *TextWeights
}

// SemanticConfiguration - Defines a specific configuration to be used in the context of semantic capabilities.
type SemanticConfiguration struct {
	// REQUIRED; The name of the semantic configuration.
	Name *string

	// REQUIRED; Describes the title, content, and keyword fields to be used for semantic ranking, captions, highlights, and answers.
// At least one of the three sub properties (titleField, prioritizedKeywordsFields and
// prioritizedContentFields) need to be set.
	PrioritizedFields *SemanticPrioritizedFields
}

// SemanticField - A field that is used as part of the semantic configuration.
type SemanticField struct {
	// REQUIRED
	FieldName *string
}

// SemanticPrioritizedFields - Describes the title, content, and keywords fields to be used for semantic ranking, captions,
// highlights, and answers.
type SemanticPrioritizedFields struct {
	// Defines the content fields to be used for semantic ranking, captions, highlights, and answers. For the best result, the
// selected fields should contain text in natural language form. The order of the
// fields in the array represents their priority. Fields with lower priority may get truncated if the content is long.
	ContentFields []*SemanticField

	// Defines the keyword fields to be used for semantic ranking, captions, highlights, and answers. For the best result, the
// selected fields should contain a list of keywords. The order of the fields in
// the array represents their priority. Fields with lower priority may get truncated if the content is long.
	KeywordsFields []*SemanticField

	// Defines the title field to be used for semantic ranking, captions, highlights, and answers. If you don't have a title field
// in your index, leave this blank.
	TitleField *SemanticField
}

// SemanticSearch - Defines parameters for a search index that influence semantic capabilities.
type SemanticSearch struct {
	// The semantic configurations for the index.
	Configurations []*SemanticConfiguration

	// Allows you to set the name of a default semantic configuration in your index, making it optional to pass it on as a query
// parameter every time.
	DefaultConfigurationName *string
}

// SentimentSkill - This skill is deprecated. Use the V3.SentimentSkill instead.
type SentimentSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *SentimentSkillLanguage

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type SentimentSkill.
func (s *SentimentSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: s.Context,
		Description: s.Description,
		Inputs: s.Inputs,
		Name: s.Name,
		ODataType: s.ODataType,
		Outputs: s.Outputs,
	}
}

// SentimentSkillV3 - Using the Text Analytics API, evaluates unstructured text and for each record, provides sentiment labels
// (such as "negative", "neutral" and "positive") based on the highest confidence score found by
// the service at a sentence and document-level.
type SentimentSkillV3 struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// If set to true, the skill output will include information from Text Analytics for opinion mining, namely targets (nouns
// or verbs) and their associated assessment (adjective) in the text. Default is
// false.
	IncludeOpinionMining *bool

	// The version of the model to use when calling the Text Analytics service. It will default to the latest available when not
// specified. We recommend you do not specify this value unless absolutely
// necessary.
	ModelVersion *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type SentimentSkillV3.
func (s *SentimentSkillV3) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: s.Context,
		Description: s.Description,
		Inputs: s.Inputs,
		Name: s.Name,
		ODataType: s.ODataType,
		Outputs: s.Outputs,
	}
}

// ServiceCounters - Represents service-level resource counters and quotas.
type ServiceCounters struct {
	// REQUIRED; Total number of data sources.
	DataSourceCounter *ResourceCounter

	// REQUIRED; Total number of documents across all indexes in the service.
	DocumentCounter *ResourceCounter

	// REQUIRED; Total number of indexes.
	IndexCounter *ResourceCounter

	// REQUIRED; Total number of indexers.
	IndexerCounter *ResourceCounter

	// REQUIRED; Total number of skillsets.
	SkillsetCounter *ResourceCounter

	// REQUIRED; Total size of used storage in bytes.
	StorageSizeCounter *ResourceCounter

	// REQUIRED; Total number of synonym maps.
	SynonymMapCounter *ResourceCounter

	// REQUIRED; Total memory consumption of all vector indexes within the service, in bytes.
	VectorIndexSizeCounter *ResourceCounter
}

// ServiceLimits - Represents various service level limits.
type ServiceLimits struct {
	// The maximum number of fields of type Collection(Edm.ComplexType) allowed in an index.
	MaxComplexCollectionFieldsPerIndex *int32

	// The maximum number of objects in complex collections allowed per document.
	MaxComplexObjectsInCollectionsPerDocument *int32

	// The maximum depth which you can nest sub-fields in an index, including the top-level complex field. For example, a/b/c
// has a nesting depth of 3.
	MaxFieldNestingDepthPerIndex *int32

	// The maximum allowed fields per index.
	MaxFieldsPerIndex *int32

	// The maximum amount of storage in bytes allowed per index.
	MaxStoragePerIndexInBytes *int64
}

// ServiceStatistics - Response from a get service statistics request. If successful, it includes service level counters and
// limits.
type ServiceStatistics struct {
	// REQUIRED; Service level resource counters.
	Counters *ServiceCounters

	// REQUIRED; Service level general limits.
	Limits *ServiceLimits
}

// ShaperSkill - A skill for reshaping the outputs. It creates a complex type to support composite fields (also known as multipart
// fields).
type ShaperSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type ShaperSkill.
func (s *ShaperSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: s.Context,
		Description: s.Description,
		Inputs: s.Inputs,
		Name: s.Name,
		ODataType: s.ODataType,
		Outputs: s.Outputs,
	}
}

// ShingleTokenFilter - Creates combinations of tokens as a single token. This token filter is implemented using Apache Lucene.
type ShingleTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The string to insert for each position at which there is no token. Default is an underscore ("_").
	FilterToken *string

	// The maximum shingle size. Default and minimum value is 2.
	MaxShingleSize *int32

	// The minimum shingle size. Default and minimum value is 2. Must be less than the value of maxShingleSize.
	MinShingleSize *int32

	// A value indicating whether the output stream will contain the input tokens (unigrams) as well as shingles. Default is true.
	OutputUnigrams *bool

	// A value indicating whether to output unigrams for those times when no shingles are available. This property takes precedence
// when outputUnigrams is set to false. Default is false.
	OutputUnigramsIfNoShingles *bool

	// The string to use when joining adjacent tokens to form a shingle. Default is a single space (" ").
	TokenSeparator *string
}

// GetTokenFilter implements the TokenFilterClassification interface for type ShingleTokenFilter.
func (s *ShingleTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: s.Name,
		ODataType: s.ODataType,
	}
}

// Similarity - Base type for similarity algorithms. Similarity algorithms are used to calculate scores that tie queries to
// documents. The higher the score, the more relevant the document is to that specific query.
// Those scores are used to rank the search results.
type Similarity struct {
	// REQUIRED
	ODataType *string
}

// GetSimilarity implements the SimilarityClassification interface for type Similarity.
func (s *Similarity) GetSimilarity() *Similarity { return s }

// SnowballTokenFilter - A filter that stems words using a Snowball-generated stemmer. This token filter is implemented using
// Apache Lucene.
type SnowballTokenFilter struct {
	// REQUIRED; The language to use.
	Language *SnowballTokenFilterLanguage

	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string
}

// GetTokenFilter implements the TokenFilterClassification interface for type SnowballTokenFilter.
func (s *SnowballTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: s.Name,
		ODataType: s.ODataType,
	}
}

// SoftDeleteColumnDeletionDetectionPolicy - Defines a data deletion detection policy that implements a soft-deletion strategy.
// It determines whether an item should be deleted based on the value of a designated 'soft delete' column.
type SoftDeleteColumnDeletionDetectionPolicy struct {
	// REQUIRED; A URI fragment specifying the type of data deletion detection policy.
	ODataType *string

	// The name of the column to use for soft-deletion detection.
	SoftDeleteColumnName *string

	// The marker value that identifies an item as deleted.
	SoftDeleteMarkerValue *string
}

// GetDataDeletionDetectionPolicy implements the DataDeletionDetectionPolicyClassification interface for type SoftDeleteColumnDeletionDetectionPolicy.
func (s *SoftDeleteColumnDeletionDetectionPolicy) GetDataDeletionDetectionPolicy() *DataDeletionDetectionPolicy {
	return &DataDeletionDetectionPolicy{
		ODataType: s.ODataType,
	}
}

// SplitSkill - A skill to split a string into chunks of text.
type SplitSkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// A value indicating which language code to use. Default is en.
	DefaultLanguageCode *SplitSkillLanguage

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The desired maximum page length. Default is 10000.
	MaximumPageLength *int32

	// Only applicable when textSplitMode is set to 'pages'. If specified, the SplitSkill will discontinue splitting after processing
// the first 'maximumPagesToTake' pages, in order to improve performance
// when only a few initial pages are needed from each document.
	MaximumPagesToTake *int32

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string

	// Only applicable when textSplitMode is set to 'pages'. If specified, n+1th chunk will start with this number of characters/tokens
// from the end of the nth chunk.
	PageOverlapLength *int32

	// A value indicating which split mode to perform.
	TextSplitMode *TextSplitMode
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type SplitSkill.
func (s *SplitSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: s.Context,
		Description: s.Description,
		Inputs: s.Inputs,
		Name: s.Name,
		ODataType: s.ODataType,
		Outputs: s.Outputs,
	}
}

// StemmerOverrideTokenFilter - Provides the ability to override other stemming filters with custom dictionary-based stemming.
// Any dictionary-stemmed terms will be marked as keywords so that they will not be stemmed with stemmers
// down the chain. Must be placed before any stemming filters. This token filter is implemented using Apache Lucene.
type StemmerOverrideTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// REQUIRED; A list of stemming rules in the following format: "word => stem", for example: "ran => run".
	Rules []*string
}

// GetTokenFilter implements the TokenFilterClassification interface for type StemmerOverrideTokenFilter.
func (s *StemmerOverrideTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: s.Name,
		ODataType: s.ODataType,
	}
}

// StemmerTokenFilter - Language specific stemming filter. This token filter is implemented using Apache Lucene.
type StemmerTokenFilter struct {
	// REQUIRED; The language to use.
	Language *StemmerTokenFilterLanguage

	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string
}

// GetTokenFilter implements the TokenFilterClassification interface for type StemmerTokenFilter.
func (s *StemmerTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: s.Name,
		ODataType: s.ODataType,
	}
}

// StopAnalyzer - Divides text at non-letters; Applies the lowercase and stopword token filters. This analyzer is implemented
// using Apache Lucene.
type StopAnalyzer struct {
	// REQUIRED; The name of the analyzer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of analyzer.
	ODataType *string

	// A list of stopwords.
	Stopwords []*string
}

// GetLexicalAnalyzer implements the LexicalAnalyzerClassification interface for type StopAnalyzer.
func (s *StopAnalyzer) GetLexicalAnalyzer() *LexicalAnalyzer {
	return &LexicalAnalyzer{
		Name: s.Name,
		ODataType: s.ODataType,
	}
}

// StopwordsTokenFilter - Removes stop words from a token stream. This token filter is implemented using Apache Lucene.
type StopwordsTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// A value indicating whether to ignore case. If true, all words are converted to lower case first. Default is false.
	IgnoreCase *bool

	// A value indicating whether to ignore the last search term if it's a stop word. Default is true.
	RemoveTrailingStopWords *bool

	// The list of stopwords. This property and the stopwords list property cannot both be set.
	Stopwords []*string

	// A predefined list of stopwords to use. This property and the stopwords property cannot both be set. Default is English.
	StopwordsList *StopwordsList
}

// GetTokenFilter implements the TokenFilterClassification interface for type StopwordsTokenFilter.
func (s *StopwordsTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: s.Name,
		ODataType: s.ODataType,
	}
}

// Suggester - Defines how the Suggest API should apply to a group of fields in the index.
type Suggester struct {
	// REQUIRED; The name of the suggester.
	Name *string

	// CONSTANT; A value indicating the capabilities of the suggester.
// Field has constant value "analyzingInfixMatching", any specified value is ignored.
	SearchMode *string

	// REQUIRED; The list of field names to which the suggester applies. Each field must be searchable.
	SourceFields []*string
}

// SynonymMap - Represents a synonym map definition.
type SynonymMap struct {
	// CONSTANT; The format of the synonym map. Only the 'solr' format is currently supported.
// Field has constant value "solr", any specified value is ignored.
	Format *string

	// REQUIRED; The name of the synonym map.
	Name *string

	// REQUIRED; A series of synonym rules in the specified synonym map format. The rules must be separated by newlines.
	Synonyms *string

	// The ETag of the synonym map.
	ETag *string

	// A description of an encryption key that you create in Azure Key Vault. This key is used to provide an additional level
// of encryption-at-rest for your data when you want full assurance that no one, not
// even Microsoft, can decrypt your data. Once you have encrypted your data, it will always remain encrypted. The search service
// will ignore attempts to set this property to null. You can change this
// property as needed if you want to rotate your encryption key; Your data will be unaffected. Encryption with customer-managed
// keys is not available for free search services, and is only available for
// paid services created on or after January 1, 2019.
	EncryptionKey *ResourceEncryptionKey
}

// SynonymTokenFilter - Matches single or multi-word synonyms in a token stream. This token filter is implemented using Apache
// Lucene.
type SynonymTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// REQUIRED; A list of synonyms in following one of two formats: 1. incredible, unbelievable, fabulous => amazing - all terms
// on the left side of => symbol will be replaced with all terms on its right side; 2.
// incredible, unbelievable, fabulous, amazing - comma separated list of equivalent words. Set the expand option to change
// how this list is interpreted.
	Synonyms []*string

	// A value indicating whether all words in the list of synonyms (if => notation is not used) will map to one another. If true,
// all words in the list of synonyms (if => notation is not used) will map to
// one another. The following list: incredible, unbelievable, fabulous, amazing is equivalent to: incredible, unbelievable,
// fabulous, amazing => incredible, unbelievable, fabulous, amazing. If false, the
// following list: incredible, unbelievable, fabulous, amazing will be equivalent to: incredible, unbelievable, fabulous,
// amazing => incredible. Default is true.
	Expand *bool

	// A value indicating whether to case-fold input for matching. Default is false.
	IgnoreCase *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type SynonymTokenFilter.
func (s *SynonymTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: s.Name,
		ODataType: s.ODataType,
	}
}

// TagScoringFunction - Defines a function that boosts scores of documents with string values matching a given list of tags.
type TagScoringFunction struct {
	// REQUIRED; A multiplier for the raw score. Must be a positive number not equal to 1.0.
	Boost *float64

	// REQUIRED; The name of the field used as input to the scoring function.
	FieldName *string

	// REQUIRED; Parameter values for the tag scoring function.
	Parameters *TagScoringParameters

	// REQUIRED; Indicates the type of function to use. Valid values include magnitude, freshness, distance, and tag. The function
// type must be lower case.
	Type *string

	// A value indicating how boosting will be interpolated across document scores; defaults to "Linear".
	Interpolation *ScoringFunctionInterpolation
}

// GetScoringFunction implements the ScoringFunctionClassification interface for type TagScoringFunction.
func (t *TagScoringFunction) GetScoringFunction() *ScoringFunction {
	return &ScoringFunction{
		Boost: t.Boost,
		FieldName: t.FieldName,
		Interpolation: t.Interpolation,
		Type: t.Type,
	}
}

// TagScoringParameters - Provides parameter values to a tag scoring function.
type TagScoringParameters struct {
	// REQUIRED; The name of the parameter passed in search queries to specify the list of tags to compare against the target
// field.
	TagsParameter *string
}

// TextTranslationSkill - A skill to translate text from one language to another.
type TextTranslationSkill struct {
	// REQUIRED; The language code to translate documents into for documents that don't specify the to language explicitly.
	DefaultToLanguageCode *TextTranslationSkillLanguage

	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// The language code to translate documents from for documents that don't specify the from language explicitly.
	DefaultFromLanguageCode *TextTranslationSkillLanguage

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string

	// The language code to translate documents from when neither the fromLanguageCode input nor the defaultFromLanguageCode parameter
// are provided, and the automatic language detection is unsuccessful.
// Default is en.
	SuggestedFrom *TextTranslationSkillLanguage
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type TextTranslationSkill.
func (t *TextTranslationSkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: t.Context,
		Description: t.Description,
		Inputs: t.Inputs,
		Name: t.Name,
		ODataType: t.ODataType,
		Outputs: t.Outputs,
	}
}

// TextWeights - Defines weights on index fields for which matches should boost scoring in search queries.
type TextWeights struct {
	// REQUIRED; The dictionary of per-field weights to boost document scoring. The keys are field names and the values are the
// weights for each field.
	Weights map[string]*float64
}

// TokenFilter - Base type for token filters.
type TokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string
}

// GetTokenFilter implements the TokenFilterClassification interface for type TokenFilter.
func (t *TokenFilter) GetTokenFilter() *TokenFilter { return t }

// TruncateTokenFilter - Truncates the terms to a specific length. This token filter is implemented using Apache Lucene.
type TruncateTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// The length at which terms will be truncated. Default and maximum is 300.
	Length *int32
}

// GetTokenFilter implements the TokenFilterClassification interface for type TruncateTokenFilter.
func (t *TruncateTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: t.Name,
		ODataType: t.ODataType,
	}
}

// UaxURLEmailTokenizer - Tokenizes urls and emails as one token. This tokenizer is implemented using Apache Lucene.
type UaxURLEmailTokenizer struct {
	// REQUIRED; The name of the tokenizer. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of tokenizer.
	ODataType *string

	// The maximum token length. Default is 255. Tokens longer than the maximum length are split. The maximum token length that
// can be used is 300 characters.
	MaxTokenLength *int32
}

// GetLexicalTokenizer implements the LexicalTokenizerClassification interface for type UaxURLEmailTokenizer.
func (u *UaxURLEmailTokenizer) GetLexicalTokenizer() *LexicalTokenizer {
	return &LexicalTokenizer{
		Name: u.Name,
		ODataType: u.ODataType,
	}
}

// UniqueTokenFilter - Filters out tokens with same text as the previous token. This token filter is implemented using Apache
// Lucene.
type UniqueTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// A value indicating whether to remove duplicates only at the same position. Default is false.
	OnlyOnSamePosition *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type UniqueTokenFilter.
func (u *UniqueTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: u.Name,
		ODataType: u.ODataType,
	}
}

// VectorSearch - Contains configuration options related to vector search.
type VectorSearch struct {
	// Contains configuration options specific to the algorithm used during indexing or querying.
	Algorithms []VectorSearchAlgorithmConfigurationClassification

	// Contains configuration options specific to the compression method used during indexing or querying.
	Compressions []VectorSearchCompressionConfigurationClassification

	// Defines combinations of configurations to use with vector search.
	Profiles []*VectorSearchProfile

	// Contains configuration options on how to vectorize text vector queries.
	Vectorizers []VectorSearchVectorizerClassification
}

// VectorSearchAlgorithmConfiguration - Contains configuration options specific to the algorithm used during indexing or querying.
type VectorSearchAlgorithmConfiguration struct {
	// REQUIRED; The name of the kind of algorithm being configured for use with vector search.
	Kind *VectorSearchAlgorithmKind

	// REQUIRED; The name to associate with this particular configuration.
	Name *string
}

// GetVectorSearchAlgorithmConfiguration implements the VectorSearchAlgorithmConfigurationClassification interface for type
// VectorSearchAlgorithmConfiguration.
func (v *VectorSearchAlgorithmConfiguration) GetVectorSearchAlgorithmConfiguration() *VectorSearchAlgorithmConfiguration { return v }

// VectorSearchCompressionConfiguration - Contains configuration options specific to the compression method used during indexing
// or querying.
type VectorSearchCompressionConfiguration struct {
	// REQUIRED; The name of the kind of compression method being configured for use with vector search.
	Kind *VectorSearchCompressionKind

	// REQUIRED; The name to associate with this particular configuration.
	Name *string

	// Default oversampling factor. Oversampling will internally request more documents (specified by this multiplier) in the
// initial search. This increases the set of results that will be reranked using
// recomputed similarity scores from full-precision vectors. Minimum value is 1, meaning no oversampling (1x). This parameter
// can only be set when rerankWithOriginalVectors is true. Higher values improve
// recall at the expense of latency.
	DefaultOversampling *float64

	// If set to true, once the ordered set of results calculated using compressed vectors are obtained, they will be reranked
// again by recalculating the full-precision similarity scores. This will improve
// recall at the expense of latency.
	RerankWithOriginalVectors *bool
}

// GetVectorSearchCompressionConfiguration implements the VectorSearchCompressionConfigurationClassification interface for
// type VectorSearchCompressionConfiguration.
func (v *VectorSearchCompressionConfiguration) GetVectorSearchCompressionConfiguration() *VectorSearchCompressionConfiguration { return v }

// VectorSearchProfile - Defines a combination of configurations to use with vector search.
type VectorSearchProfile struct {
	// REQUIRED; The name of the vector search algorithm configuration that specifies the algorithm and optional parameters.
	AlgorithmConfigurationName *string

	// REQUIRED; The name to associate with this particular vector search profile.
	Name *string

	// The name of the compression method configuration that specifies the compression method and optional parameters.
	CompressionConfigurationName *string

	// The name of the vectorization being configured for use with vector search.
	VectorizerName *string
}

// VectorSearchVectorizer - Specifies the vectorization method to be used during query time.
type VectorSearchVectorizer struct {
	// REQUIRED; The name of the kind of vectorization method being configured for use with vector search.
	Kind *VectorSearchVectorizerKind

	// REQUIRED; The name to associate with this particular vectorization method.
	Name *string
}

// GetVectorSearchVectorizer implements the VectorSearchVectorizerClassification interface for type VectorSearchVectorizer.
func (v *VectorSearchVectorizer) GetVectorSearchVectorizer() *VectorSearchVectorizer { return v }

// WebAPIParameters - Specifies the properties for connecting to a user-defined vectorizer.
type WebAPIParameters struct {
	// The user-assigned managed identity used for outbound connections. If an authResourceId is provided and it's not specified,
// the system-assigned managed identity is used. On updates to the indexer, if
// the identity is unspecified, the value remains unchanged. If set to "none", the value of this property is cleared.
	AuthIdentity IndexerDataIdentityClassification

	// Applies to custom endpoints that connect to external code in an Azure function or some other application that provides
// the transformations. This value should be the application ID created for the
// function or app when it was registered with Azure Active Directory. When specified, the vectorization connects to the function
// or app using a managed ID (either system or user-assigned) of the search
// service and the access token of the function or app, using this value as the resource id for creating the scope of the
// access token.
	AuthResourceID *string

	// The headers required to make the HTTP request.
	HTTPHeaders map[string]*string

	// The method for the HTTP request.
	HTTPMethod *string

	// The desired timeout for the request. Default is 30 seconds.
	Timeout *string

	// The URI of the Web API providing the vectorizer.
	URI *string
}

// WebAPISkill - A skill that can call a Web API endpoint, allowing you to extend a skillset by having it call your custom
// code.
type WebAPISkill struct {
	// REQUIRED; Inputs of the skills could be a column in the source data set, or the output of an upstream skill.
	Inputs []*InputFieldMappingEntry

	// REQUIRED; A URI fragment specifying the type of skill.
	ODataType *string

	// REQUIRED; The output of a skill is either a field in a search index, or a value that can be consumed as an input by another
// skill.
	Outputs []*OutputFieldMappingEntry

	// REQUIRED; The url for the Web API.
	URI *string

	// The user-assigned managed identity used for outbound connections. If an authResourceId is provided and it's not specified,
// the system-assigned managed identity is used. On updates to the indexer, if
// the identity is unspecified, the value remains unchanged. If set to "none", the value of this property is cleared.
	AuthIdentity IndexerDataIdentityClassification

	// Applies to custom skills that connect to external code in an Azure function or some other application that provides the
// transformations. This value should be the application ID created for the
// function or app when it was registered with Azure Active Directory. When specified, the custom skill connects to the function
// or app using a managed ID (either system or user-assigned) of the search
// service and the access token of the function or app, using this value as the resource id for creating the scope of the
// access token.
	AuthResourceID *string

	// The desired batch size which indicates number of documents.
	BatchSize *int32

	// Represents the level at which operations take place, such as the document root or document content (for example, /document
// or /document/content). The default is /document.
	Context *string

	// If set, the number of parallel calls that can be made to the Web API.
	DegreeOfParallelism *int32

	// The description of the skill which describes the inputs, outputs, and usage of the skill.
	Description *string

	// The headers required to make the http request.
	HTTPHeaders map[string]*string

	// The method for the http request.
	HTTPMethod *string

	// The name of the skill which uniquely identifies it within the skillset. A skill with no name defined will be given a default
// name of its 1-based index in the skills array, prefixed with the character
// '#'.
	Name *string

	// The desired timeout for the request. Default is 30 seconds.
	Timeout *string
}

// GetIndexerSkill implements the IndexerSkillClassification interface for type WebAPISkill.
func (w *WebAPISkill) GetIndexerSkill() *IndexerSkill {
	return &IndexerSkill{
		Context: w.Context,
		Description: w.Description,
		Inputs: w.Inputs,
		Name: w.Name,
		ODataType: w.ODataType,
		Outputs: w.Outputs,
	}
}

// WebAPIVectorizer - Specifies a user-defined vectorizer for generating the vector embedding of a query string. Integration
// of an external vectorizer is achieved using the custom Web API interface of a skillset.
type WebAPIVectorizer struct {
	// REQUIRED; The name of the kind of vectorization method being configured for use with vector search.
	Kind *VectorSearchVectorizerKind

	// REQUIRED; The name to associate with this particular vectorization method.
	Name *string

	// Specifies the properties of the user-defined vectorizer.
	WebAPIParameters *WebAPIParameters
}

// GetVectorSearchVectorizer implements the VectorSearchVectorizerClassification interface for type WebAPIVectorizer.
func (w *WebAPIVectorizer) GetVectorSearchVectorizer() *VectorSearchVectorizer {
	return &VectorSearchVectorizer{
		Kind: w.Kind,
		Name: w.Name,
	}
}

// WordDelimiterTokenFilter - Splits words into subwords and performs optional transformations on subword groups. This token
// filter is implemented using Apache Lucene.
type WordDelimiterTokenFilter struct {
	// REQUIRED; The name of the token filter. It must only contain letters, digits, spaces, dashes or underscores, can only start
// and end with alphanumeric characters, and is limited to 128 characters.
	Name *string

	// REQUIRED; A URI fragment specifying the type of token filter.
	ODataType *string

	// A value indicating whether all subword parts will be catenated. For example, if this is set to true, "Azure-Search-1" becomes
// "AzureSearch1". Default is false.
	CatenateAll *bool

	// A value indicating whether maximum runs of number parts will be catenated. For example, if this is set to true, "1-2" becomes
// "12". Default is false.
	CatenateNumbers *bool

	// A value indicating whether maximum runs of word parts will be catenated. For example, if this is set to true, "Azure-Search"
// becomes "AzureSearch". Default is false.
	CatenateWords *bool

	// A value indicating whether to generate number subwords. Default is true.
	GenerateNumberParts *bool

	// A value indicating whether to generate part words. If set, causes parts of words to be generated; for example "AzureSearch"
// becomes "Azure" "Search". Default is true.
	GenerateWordParts *bool

	// A value indicating whether original words will be preserved and added to the subword list. Default is false.
	PreserveOriginal *bool

	// A list of tokens to protect from being delimited.
	ProtectedWords []*string

	// A value indicating whether to split words on caseChange. For example, if this is set to true, "AzureSearch" becomes "Azure"
// "Search". Default is true.
	SplitOnCaseChange *bool

	// A value indicating whether to split on numbers. For example, if this is set to true, "Azure1Search" becomes "Azure" "1"
// "Search". Default is true.
	SplitOnNumerics *bool

	// A value indicating whether to remove trailing "'s" for each subword. Default is true.
	StemEnglishPossessive *bool
}

// GetTokenFilter implements the TokenFilterClassification interface for type WordDelimiterTokenFilter.
func (w *WordDelimiterTokenFilter) GetTokenFilter() *TokenFilter {
	return &TokenFilter{
		Name: w.Name,
		ODataType: w.ODataType,
	}
}

